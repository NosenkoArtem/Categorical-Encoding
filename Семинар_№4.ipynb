{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NosenkoArtem/Categorical-Encoding/blob/master/%D0%A1%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80_%E2%84%964.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b713eb0",
      "metadata": {
        "id": "4b713eb0"
      },
      "source": [
        "# Семинар 4. Непрерывные диффузионные модели"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " *Автор*: Александр Колесов"
      ],
      "metadata": {
        "id": "CRyurfMwbWYv"
      },
      "id": "CRyurfMwbWYv"
    },
    {
      "cell_type": "markdown",
      "id": "d4a06389",
      "metadata": {
        "id": "d4a06389"
      },
      "source": [
        "## 1. Мотивация"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de65bfc3",
      "metadata": {
        "id": "de65bfc3"
      },
      "source": [
        "$\\textbf{Вопрос:}$\n",
        "какую задачу на протяжении семинара 2 (методы оценивания score-функций) мы пытались решить различными способами — от минимизации дивергенции Фишера до Denoising score matching?\n",
        "\n",
        "Это была задача, в которой по выборке из расспределения $p(x)$ нужно было оценить $\\nabla_{x}\\log p(x)$. Сейчас пришло время раскрыть мотивацию: почему это так необходимо в диффузионных моделях?\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "какой процесс производил зашумление данных, рассмотренное во время семинара 3, и каков его вид?\n",
        "\n",
        "$\\textbf{Мотивация:}$\n",
        "\n",
        "- Зашумление данных в дискретной модели проходило при помощи уравнения $x_{t+1} = \\sqrt{1-\\beta_{t}}x_{t} + \\sqrt{\\beta_{t}}\\epsilon , \\quad \\epsilon \\sim \\mathcal{N}(0,I).$\n",
        "\n",
        "- Чуть ниже вы узнаете, что это стохастическое дифференциальное уравнение (СДУ), то есть такой процесс можно формально записать как $$dx_{t} = f(x_{t},t)dt + G(t)dw_{t}.$$\n",
        "\n",
        "- Помните, во время семинара 3 (Дискретные диффузионные модели (DDPM)) мы сказали, что развернуть по времени ОДУ можно, а вот СДУ — нет. Поэтому матчим средние прямого и обратного процессов.\n",
        "\n",
        "- А что если такой инструмент разворота существует? Это так — теорема Андерсона. Согласно этой теореме обратный процесс выглядит так:\n",
        "$$ dx_{t} = [f(x_{t},t) - \\frac{1}{2}G(t)G(t)^{T}\\nabla_{x} \\log p_{t}(x_{t})]dt + G(t)dw_{t}.  $$\n",
        "\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "прямой процесс в диффузионных моделях обучаемый или нет?\n",
        "\n",
        "Поскольку в диффузионных моделях прямой процесс необучаемый, $f(x_{t},t)$ и $G(t)$ мы выбираем сами, и они являются фиксированными. Тогда, наблюдая СДУ для обратного процесса, мы видим, что СДУ использует те же $f(x_{t},t)$ и $G(t)$, а единственный ингредиент, который отличает СДУ от прямого и обратного процессов, — это $\\nabla_{x} \\log p_{t}(x_{t}).$\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "что стоит за $\\nabla_{x} \\log p_{t}(x_{t})$ с точки зрения процесса?\n",
        "\n",
        "Итак, если мы научимся вычислять в каждый момент времени $\\nabla_{x} \\log p_{t}(x_{t})$, то сможем запускать обратное СДУ и генерировать картинки из шума. Как раз то, что нужно!\n",
        "\n",
        "$\\textbf{Вопрос к аудитории:}$\n",
        "какими методами можно оценить $\\nabla_{x} \\log p_{t}(x_{t})$ ? В чем недостатки таких подходов?\n",
        "\n",
        "![title](https://drive.google.com/uc?id=1CGFbtY2mCjlIY8pjvoGevfa_32d4b1dj)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Безусловно, для решения задачи оценки score-функции можно использовать:\n",
        "\n",
        "- Implicit Score matching;\n",
        "- Denosing Score matching;\n",
        "- NCSN.\n",
        "\n",
        "Однако рассмотрим один произвольный момент времени $t$ и выпишем задачу поиска score-функции:\n",
        "\n",
        "$$ \\int p_{t}(x_{t}) || s_{\\theta}(x_{t},t)  - \\nabla_{x}\\log p_{t}(x_{t}) ||_{2}^{2} dx_{t} \\to \\min_{\\theta}.$$\n",
        "\n",
        "Сделаем это так же, как часто расписывали в семинаре 2:\n",
        "\n",
        "$$ \\mathbb{E}_{p_{t}(x_{t})}|| s_{\\theta}(x,t)||^{2}_{2} - 2 \\int < s_{\\theta}(x,t),\\nabla_{x} p_{t}(x_{t})>dx_{t}.$$\n",
        "\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "как можно представить маргинальное распределение в любой момент времени $p_{t}(x_{t})$ через нулевой момент времени?\n",
        "\n",
        "Обратите внимание на простой факт:\n",
        "$$p_{t}(x_{t}) = \\int p_{t}(x_{t}, x_{0})dx_{0} = \\int q(x_{t}|x_{0})p_{0}(x_{0})dx_{0}.  $$\n",
        "\n",
        "Тогда наше выражение перепишется так:\n",
        "\n",
        "$$ \\mathbb{E}||s_{\\theta}(x,t)||^{2}_{2} - 2\\int_{x_{t}} \\int_{x_{0}} p_{0}(x_{0})<s_{\\theta}(x,t),\\nabla_{x}q(x_{t}|x_{0})>dx_{t}.$$\n",
        "\n",
        "И теперь, если мы знаем $\\nabla_{x}q(x_{t}|x_{0})$, то можем вычислить это выражение, а значит, посчитать задачу оптимизации.\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "как в более простом виде, с учетом выкладок выше, будет выглядеть задача оптимизации и какой из методов семинара 2 она будет больше напоминать?\n",
        "\n",
        "$$\\mathbb{E}_{p_{t}(x_{t})} || s_{\\theta}(x_{t},t)- \\nabla_{x} \\log q_{t}(x_{t}|x_{0})||^{2}_{2} \\to \\min_{\\theta}.$$"
      ],
      "metadata": {
        "id": "rX3e9DLjcXi_"
      },
      "id": "rX3e9DLjcXi_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02fcdc1",
      "metadata": {
        "id": "e02fcdc1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import (\n",
        "    Resize,\n",
        "    Normalize,\n",
        "    Compose,\n",
        "    RandomHorizontalFlip,\n",
        "    ToTensor,\n",
        "    Lambda,\n",
        "    CenterCrop\n",
        ")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import wandb\n",
        "import os\n",
        "import math\n",
        "import functools\n",
        "import string\n",
        "\n",
        "from ml_collections import ConfigDict\n",
        "from typing import Optional, Union, Callable\n",
        "from tqdm.auto import trange\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from  ContDDPM.configs.default_cm_2_config import create_default_cm_2_config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d4e05c",
      "metadata": {
        "id": "b3d4e05c"
      },
      "source": [
        "### 1.1. Вывод непрерывного прямого диффузионного процесса из дискретного"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e330e49a",
      "metadata": {
        "id": "e330e49a"
      },
      "source": [
        "Теперь построим связующий мостик между семинарами 3 и 4.\n",
        "\n",
        "\n",
        "1. Рассмотрим простую дискретную схему:\n",
        "\n",
        "$$x_{i} = (1 - \\frac{\\beta \\Delta t}{2})x_{i-1} .$$\n",
        "\n",
        "$\\textbf{Наша цель}$ — записать непрерывный аналог данной дискретной схемы:\n",
        "\n",
        "- $x_{i} = x(\\frac{i}{N});$\n",
        "- $\\Delta t = \\frac{1}{N};$\n",
        "- $t \\in \\{0,\\frac{1}{N},...,\\frac{N-1}{N} \\};$\n",
        "\n",
        "$$ x(t + \\Delta t) = (1 - \\frac{\\beta \\Delta t}{2})x(t);$$\n",
        "\n",
        "$$ \\frac{ x(t + \\Delta t) - x(t)}{\\Delta t} = - \\frac{\\beta}{2}x(t).$$\n",
        "\n",
        "Тогда в непрерывном случае при маленьких значениях $\\Delta t$ процесс имеет вид\n",
        "\n",
        "$$ \\frac{dx(t)}{dt} = - \\frac{\\beta}{2}x(t).$$\n",
        "\n",
        "2. Рассмотрим следующую дискретную схему:\n",
        "\n",
        "$$x_{i} = x_{i-1} - \\beta_{i-1}\\nabla f(x_{i-1}). $$\n",
        "\n",
        "Отсутствие $\\Delta t $ мешает нам пока записать непрерывный аналог данного выражения, потому давайте введем его следующим образом:\n",
        "\n",
        "$$ \\beta_{i-1} = \\beta(t)\\Delta t.$$\n",
        "\n",
        "Тогда перепишем:\n",
        "\n",
        "$$ \\frac{x(t+\\Delta t) - x(t)}{\\Delta t} = - \\beta(t)\\nabla f(x(t)).$$\n",
        "\n",
        "И тогда непрерывная запись выглядит как\n",
        "\n",
        "$$ \\frac{dx(t)}{dt} =  - \\beta(t)\\nabla f(x(t)). $$\n",
        "\n",
        "3. Рассмотрим один шаг дискретного DDPM между двумя произвольными соседними моментами времени $i-1$  и $i$:\n",
        "\n",
        "$$ x_{i} = \\sqrt{1 - \\beta_{i}}x_{i-1} +\\sqrt{\\beta_{i}}\\epsilon_{i-1}, \\quad \\epsilon_{i-1} \\sim \\mathcal{N}(0,I).$$\n",
        "\n",
        "Поскольку нам нужно перейти от дискретного случая к непрерывному, понятие приращения времени должно фигурировать в $dt = \\frac{1}{N}$, где $N$ — число шагов диффузионного процесса. Поэтому масштабируем расписание шума:\n",
        "\n",
        "- Ранее: $\\beta_{0} = 0.02, \\beta_{N} = 1.$\n",
        "\n",
        "- Сейчас: $\\overline{\\beta_{0}} = N \\beta_{0} , ..., \\overline{\\beta_{N}} = N \\beta_{N} \\implies \\{ \\overline\\beta_{i} =  N\\beta_{i}\\}_{i=0}^{N}.$\n",
        "\n",
        "Такое перемасштабирование шума позволяет определить текущее значение $\\beta_{i}$ шума как $\\beta_{i} = \\frac{\\overline{\\beta_{i}}} {N}.$\n",
        "\n",
        "Тогда можем переписать выражение выше:\n",
        "\n",
        "$$ x_{i} = \\sqrt{1 - \\frac{\\overline{\\beta_{i}}}{N}}x_{i-1} + \\sqrt{\\frac{\\overline{\\beta_{i}}}{N}}\\epsilon_{i-1} . $$\n",
        "\n",
        "И при стремлении $N \\to \\infty$ полагаем\n",
        "\n",
        "- $dt = \\frac{1}{N};$\n",
        "- $\\{\\overline{\\beta_{i}}\\}_{i=1}^{N}$ становится функцией от времени $\\beta(t);$\n",
        "- $\\beta(\\frac{i}{N}) = \\overline{\\beta_{i}};$\n",
        "- $x(\\frac{i}{N}) = x_{i};$\n",
        "- $\\epsilon(\\frac{i}{N}){i} = \\epsilon_{i} .$\n",
        "\n",
        "С учетом последнего перепишем крайнее выражение:\n",
        "\n",
        "$$ x_{i} = \\sqrt{1 -   \\beta\\left(\\frac{i}{N}\\right)dt}x_{i-1} + \\sqrt{  \\beta\\left(\\frac{i}{N}\\right)dt}\\epsilon_{i-1}.$$\n",
        "\n",
        "Переходя от индексов к времени  $t = \\frac{i}{N}$:\n",
        "\n",
        "$$ x(t + dt) = \\sqrt{1 -   \\beta(t +dt)dt}x(t) + \\sqrt{  \\beta(t+dt)dt}\\epsilon(t).$$\n",
        "\n",
        "Используем формулу Тейлора для квадратичного корня, поскольку $\\beta \\in [0,1]:$\n",
        "\n",
        "$$ x(t + dt) \\approx (1 - \\frac{1}{2}\\beta(t+dt)dt)x(t)  + \\sqrt{\\beta(t+dt)dt}\\epsilon(t),$$\n",
        "\n",
        "$$ x(t + dt) - x(t) = dx \\approx - \\frac{1}{2}\\beta(t)x(t)dt + \\sqrt{\\beta(t)dt}\\epsilon(t) .$$\n",
        "\n",
        "Тогда  $dw_{t} = \\sqrt{dt}\\epsilon(t),$\n",
        "\n",
        "$$dx = -\\frac{1}{2}\\beta(t)x(t)dt + \\sqrt{\\beta(t)}dw_{t}.$$\n",
        "\n",
        "![ChessUrl]( https://stable-diffusion-art.com/wp-content/uploads/2022/12/image-79.png \"chess\")  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Не забудь обьяснить про приращение Винеровского что это корень времени"
      ],
      "metadata": {
        "id": "1ERGrDB5gyAP"
      },
      "id": "1ERGrDB5gyAP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b018c22f",
      "metadata": {
        "id": "b018c22f"
      },
      "source": [
        "### 1.2.  Теорема Андерсона"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c20091d",
      "metadata": {
        "id": "7c20091d"
      },
      "source": [
        "Теперь вы умеете определять $\\textbf{прямой}$ диффузионный процесс в прямом направлении от данных к шуму. И наша $\\textbf{главная задача}$, как и в предыдущем семинаре, в том, чтобы на основе прямого процесса построить процесс $\\textbf{обратный}$. Он как раз и направлен на генерацию данных из шума.\n",
        "\n",
        "Тогда мы задаемся вопросом:  $\\textbf{Как, имея прямой процесс, построить обратный в непрерывном случае?}$\n",
        "\n",
        "Ответ на этот вопрос и дает теорема Андерсона.\n",
        "\n",
        " $\\textbf{Теорема Андерсона}$(1982).\n",
        "Пусть прямой диффузионный процесс описывается уравнением\n",
        "$$dx = f(x,t)dt + G(t)dw.$$\n",
        "Тогда соответствующий ему обратный диффузионный процесс задается уравнением\n",
        "$$dx = [f(x,t) - \\frac{1}{2} G(t) G(t)^{T}\\nabla_{x}\\log p_{t}(x)]dt + G(t)dw.$$\n",
        "\n",
        "Тогда, изучая форму обратного процесса, видим, что обратный процесс нам известен. Он имеет следующие особености:\n",
        "\n",
        "- Шумовой терм обратного процесса полностью совпадает с прямым.\n",
        "- Также знакомый дрифт прямого процесса $f(x,t)$ входит в дрифт обратного.\n",
        "- Однако появился новый член $\\nabla_{x}\\log p_{t}(x).$\n",
        "\n",
        "По форме записи мы уже прекрасно пониманием, что это выражение является $\\textbf{score-функцией}$. И теперь наша основная задача состоит в том чтобы ее найти — это ровно то, что мы разобрали в мотивации."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13670803",
      "metadata": {
        "id": "13670803"
      },
      "source": [
        "### 1.3. Основы теории стохастических дифференциальных уравнений"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3f35a0",
      "metadata": {
        "id": "2e3f35a0"
      },
      "source": [
        "#### 1.3.1. Понимание структуры вида и вывод СДУ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5694c4",
      "metadata": {
        "id": "ae5694c4"
      },
      "source": [
        "\n",
        "Как вы уже знаете, динамика Ланжевена тоже есть некоторое СДУ. Напомним, что на интуитивном уровне динамика Ланжевена представляет собой следующую конструкцию:\n",
        "\n",
        "$$ dx_{t} = f_{t} + g_{t},$$\n",
        "\n",
        "где $f_{t}$ — детерминистичная часть $\\epsilon\\nabla_{x}\\log p(x)$, в то время как $g(t) = \\sqrt{2\\epsilon}z_{i}$ — шумовой терм.\n",
        "\n",
        "$\\textbf{Вопросы:}$\n",
        "\n",
        "- За что отвечает детерминистичный терм динамики Ланжевена?\n",
        "- На какую оптимизационную процедуру похожа динамика Ланжевена без шумового слагаемого?\n",
        "- За что отвечает шумовое слагаемое в динамике Ланжевена?\n",
        "\n",
        "![title](https://s.iimg.su/s/13/HN504VnoyJeiaWvpfjlcJAjBT1UfFf6LJz6zeun7.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a792e7fb",
      "metadata": {
        "id": "a792e7fb"
      },
      "source": [
        "В показанной выше интуитивной терминологии детерминистического слагаемого и шума СДУ может быть представлено (например, для двумерного диффузионного процессса в случае $D=2$) как\n",
        "\n",
        "$$ \\frac{dx_{t}}{dt} = f(x,t) + G(x,t)w(t).$$\n",
        "\n",
        "И сразу определим, кто есть кто в этой записи. Поскольку процесс наш двумерный (для примера), значит, диффундирует по обеим координатам — как по оси $x$, так и по оси $y$. Тогда прежде всего разберемся с размерностями входящих в уравнение величин:\n",
        "\n",
        "- $dx_{t} =  \\begin{pmatrix}x^{(1)}_{t} -x^{(1)}_{t-1}\\\\x^{(2)}_{t} - x^{(2)}_{t-1}\\end{pmatrix}$, то есть это $D$-мерный вектор ($D=2$);\n",
        "- $dt$ — это тоже $D$-мерный вектор ($D=2$), что отражает приращение времени по каждой из компонент процесса;\n",
        "- $f(x_{t},t)=\\begin{pmatrix}f^{1}(x_{t},t)\\\\ f^{2}(x_{t},t\\end{pmatrix}$ — это $D$-мерный вектор дрифта ($D=2$);\n",
        "- $G(x_{t},t)$ — диффузионная матрица размера $D\\times D$ ($D=2$);\n",
        "- $w(t)=\\begin{pmatrix}w_{t}^{(1)}\\\\ w_{t}^{(2)}\\end{pmatrix}$ — это белый шум в момент времени $t$ по каждой из компонент, то есть это $D$-мерный вектор ($D=2$).\n",
        "\n",
        "Теперь стоит понять, что собственно обозначают дрифт и диффузионная матрица процесса. Какой в них смысл, что они несут?\n",
        "\n",
        "$\\textbf{Вопросы для аудитории:}$\n",
        "\n",
        "- В чем суть дрифта процесса $f(x_{t},t)$ ?\n",
        "- В чем суть диффузионной матрицы  $G(x_{t},t)$ ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31e4298",
      "metadata": {
        "id": "e31e4298"
      },
      "source": [
        "$\\textbf{Ответ}$: Если с диффузионным дрифтом все понятно (он показывает тренд, куда двигается процесс), то с сутью диффузионной матрицы дела обстоят несколько сложнее.\n",
        "\n",
        "Как мы говорили ранее, наш диффузионный процесс обладает тем свойством , что у него каждая компонента диффундирует и, безусловно, существуют какие-то корреляции между процессами по этим компонентам. Как раз такие корреляции и описывает диффузионная матрица.\n",
        "\n",
        "Например, в случае двумерного процесса диффузионная матрица\n",
        "\n",
        "$$ G(x_{t},t) = \\begin{pmatrix} dx^{(1)}_{t} dx^{(1)}_{t} &   dx^{(1)}_{t} dx^{(2)}_{t}\\\\\n",
        " dx^{(2)}_{t} dx^{(1)}_{t}& dx^{(2)}_{t} dx^{(2)}_{t}\\end{pmatrix}. $$\n",
        "\n",
        "\n",
        "Однако определенное выше якобы СДУ $\\textbf{не является}$ ДУ в общепринятом смысле, поскольку теория ДУ не допускает $\\textbf{разрывных}$ функций, таких как $w(t)$. **К чему приводит такая разрывность?**\n",
        "\n",
        "Рассмотрим проинтегрированное нами СДУ за весь период времени от $t_{0}$ до ${t}$ :\n",
        "\n",
        "$$ x(t) - x(0) = \\int_{t_{0}}^{t}f(x(t),t)dt + \\int_{t_{0}}^{t} G(x(t),t)w(t)dt.$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://upload.wikimedia.org/wikipedia/commons/9/97/RiemannInt.png)"
      ],
      "metadata": {
        "id": "pmglvJu8jkGH"
      },
      "id": "pmglvJu8jkGH"
    },
    {
      "cell_type": "markdown",
      "id": "1cd0918f",
      "metadata": {
        "id": "1cd0918f"
      },
      "source": [
        "$\\textbf{Вопросы}$\n",
        "- Являются ли оба представленных интеграла римановыми?\n",
        "- Что собой представляет интеграл Римана?\n",
        "\n",
        "Поскольку функция $f$ непрерывна от $t_{0}$ до ${t}$, она и интегрируема по Риману.\n",
        "\n",
        "А теперь давайте обратим внимание на второй интеграл, представляющий следующущю предельную сумму:\n",
        "\n",
        "$$\\int_{t_{0}}^{t} G(x(t),t)w(t)dt = \\lim_{K\\to \\infty} \\sum_{k=1}^{K} G(x(t^{*}_{k}),t^{*}_{k}) w(t^{*}_{k})(t_{k+1}-t_{k}). $$\n",
        "\n",
        "$\\textbf{Вопросы для аудитории}$\n",
        "- Откуда взяты $t^{*}_{k}$?\n",
        "- Когда интеграл Римана существует? Подсказка: вспомните про интегральные суммы — верхние и нижние.\n",
        "- Почему второй интеграл не сходится в смысле Римана?\n",
        "\n",
        "Чтобы обеспечить сходимость этого интеграла, можно перейти к интегралу Стильтеса, более подробно о котором можно почитать по ссылке https://en.wikipedia.org/wiki/Lebesgue–Stieltjes_integration\n",
        "\n",
        "$\\textbf{Самое главное}$, чтобы определить интеграл Стилтьеса, необходимо рассмотреть $w(t)dt$ как инкремент некоторого случайного процесса, коим и является винеровский процесс $dw(t)$. И таким образом мы переходим к новой записи диффузионного процесса:\n",
        "    \n",
        "$$ G(x(t),t)w(t)dt \\to G(x(t),t)dw(t).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Wiener-process-5traces.svg/1040px-Wiener-process-5traces.svg.png)"
      ],
      "metadata": {
        "id": "73gAxWHfKHDp"
      },
      "id": "73gAxWHfKHDp"
    },
    {
      "cell_type": "markdown",
      "id": "d690d3bb",
      "metadata": {
        "id": "d690d3bb"
      },
      "source": [
        "$\\textbf{Винеровский процесс:}$\n",
        "\n",
        "Винеровский процесс — это процесс независимых приращений $dw(t)$:\n",
        "\n",
        "1. $dw(t) = w(t_{k+1}) - w(t_{k}) \\sim \\mathcal{N}(0,Q dt_{k})$  c $dt_{k} = t_{k+1} -t_{k}$.\n",
        "2. $Qdt_{k}=  \\begin{pmatrix}dw(t)^{(1)}dw(t)^{(1)} & dw(t)^{(1)}dw(t)^{(2)} \\\\\n",
        "dw(t)^{(2)}dw(t)^{(1)} & dw(t)^{(2)}dw(t)^{(2)}\\end{pmatrix}$ — матрица двумерного винеровского процесса.\n",
        "3. Процесс начинается с $w(0) = 0$.\n",
        "4. Инкременты являются независимыми случайными величинами.\n",
        "\n",
        "$\\textbf{Вопрос для аудитории:}$\n",
        "- В чем физический смысл матрицы винеровского пооцесса?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992a7441",
      "metadata": {
        "id": "992a7441"
      },
      "source": [
        "Таким образом, мы пришли к финальному виду СДУ:\n",
        "    \n",
        "$$ dx(t) = f(x(t),t)dt  + G(x(t),t)dw(t).$$\n",
        "\n",
        "#### Дополнение к данной главе\n",
        "\n",
        "**Обратите внимание**, что интеграл Стилтьеса тоже расходится  при произвольном выборе точки в отрезке. Однако его заменяют похожим интегралом — интегралом Ито, который всегда выбирает нижнюю точку на отрезке. Интеграл Ито полностью решает проблему расходимости второго интеграла, однако объяснение этой темы выходит за пределы занятия.\n",
        "\n",
        "$\\textbf{Полезные сссылки:}$\n",
        "\n",
        "- Интеграл Ито: https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/ef2c66c8079ba656210ad1fd4a5e2fa8_MIT18_S096F13_lecnote18.pdf\n",
        "\n",
        "- Интеграл Стратановича: https://www.degruyter.com/document/doi/10.1515/9783110741278-022/html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ChessUrl]( https://stable-diffusion-art.com/wp-content/uploads/2022/12/image-79.png \"chess\")   "
      ],
      "metadata": {
        "id": "XmfaY8puNSpM"
      },
      "id": "XmfaY8puNSpM"
    },
    {
      "cell_type": "markdown",
      "id": "6a64ccef",
      "metadata": {
        "id": "6a64ccef"
      },
      "source": [
        "#### 1.3.2. Формула Ито"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eecc784",
      "metadata": {
        "id": "6eecc784"
      },
      "source": [
        "$\\textbf{Важно:}$ пока держим в уме нашу главную мотивацию:\n",
        "- Знать $\\textbf{истинное}$ значение для $\\nabla_{x}\\log q(x(t)|x(0))$.\n",
        "- На него обучать score-модель $s_{\\theta}(x,t)$.\n",
        "- Запускать обратное СДУ для генерации изображений.\n",
        "\n",
        "Для ответа на первый вопрос осталось познакомиться с еще одним важным инструментом — $\\textbf{формулой Ито}$.\n",
        "\n",
        "Формула Ито утверждает:  если у вас есть некоторый случайный процесс на $x(t) = x_{t}$ и вы знаете, как это процесс выглядит (например, $ dx_{t} = f(x_{t},t)dt + G(x_{t},t)dw_{t}  $ ), то вы легко поймете, как выглядит процесс на любую  $\\textbf{скалярную}$ функцию от этого процесса $\\phi(x_{t})$:\n",
        "\n",
        "$$ d\\phi = \\frac{\\partial \\phi}{\\partial t}dt + \\frac{\\partial \\phi}{\\partial x}dx + \\frac{1}{2}\\frac{\\partial^{2}\\phi}{\\partial x^{2}}dx^{2} = \\frac{\\partial \\phi}{\\partial t}dt +\\sum_{i}\\frac{\\partial \\phi}{\\partial x_{i}}dx_{i} + \\frac{1}{2}\\sum_{i,j}\\frac{\\partial^{2}\\phi}{\\partial x_{i} \\partial x_{j}}dx_{i}dx_{j} .$$\n",
        "\n",
        "Эта формула выводится в соответствии с формулой Тейлора.  \n",
        "\n",
        "$\\textbf{Вопрос:}$ выведите по формуле Ито $ \\phi(x(t)) = \\frac{1}{2}x^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0275c1f7",
      "metadata": {
        "id": "0275c1f7"
      },
      "source": [
        "$\\textbf{Сравнение приращения процесса и времени:}$\n",
        "\n",
        "- $dw(t) = \\sqrt{dt}$\n",
        "\n",
        "$\\textbf{Правила малости:}$\n",
        "\n",
        "Эти правила следуют из того, что мы рассмтрели формулу Тейлора до 2-го порядка малости:\n",
        "\n",
        "- $dw(t) dt = 0$;\n",
        "- $dt dw(t) = 0$;\n",
        "- $dw(t) dw(t) = Qdt$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407f3cd9",
      "metadata": {
        "id": "407f3cd9"
      },
      "outputs": [],
      "source": [
        "# Вывод dw(t)dw(t) = Qdt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07994d25",
      "metadata": {
        "id": "07994d25"
      },
      "source": [
        "#### 1.3.3. Формула Фоккера—Планка"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://s.iimg.su/s/14/d8udvBiQra9TdzuH8eLeVBTKIFnRhWh5blZY98rq.png)"
      ],
      "metadata": {
        "id": "Wt5w-1Yc-K8P"
      },
      "id": "Wt5w-1Yc-K8P"
    },
    {
      "cell_type": "markdown",
      "id": "4d6a7909",
      "metadata": {
        "id": "4d6a7909"
      },
      "source": [
        "Уравнение Фоккера—Планка показывает, какому закону в каждый момент времени подчиняются маргинальные распределения процесса.\n",
        "\n",
        "$\\textbf{Внимание:}$ а ведь это ровно то уравнение, которое описывает нужную нам $q(x(t)|x(0))$, а значит, зная аналитическую форму такого распределения, можно:\n",
        "\n",
        "- взять логарифм $\\log q(x(t)|x(0))$;\n",
        "- посчитать градиент $\\nabla_{x} \\log q(x(t)|x(0))$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "507ad647",
      "metadata": {
        "id": "507ad647"
      },
      "source": [
        "Давайте разберемся, откуда появляется уравнение Фоккера—Планка.\n",
        "\n",
        "1. Рассматриваем СДУ для ($D=2$)-мерного случайного процесса:\n",
        "\n",
        "$$ dx_{t} = f(x_{t},t)dt + G(x_{t},t)dw_{t}, \\quad x(t_{0}) \\sim p_{0}.$$\n",
        "\n",
        "2. Выпишем Формулу Ито:\n",
        "\n",
        "$$ d\\phi = \\frac{\\partial \\phi}{\\partial t}dt + \\sum_{i=1}\\frac{\\partial \\phi}{\\partial x_{i}}dx_{i} +\n",
        "\\frac{1}{2}\\sum_{i,j} \\frac{\\partial^{2}\\phi}{\\partial x_{i}\\partial x_{j}}dx_{i}dx_{j}.$$\n",
        "\n",
        "Считая для простоты, что потенциал не зависит от времени, выразим ранее определенную запись для произведения компонент процесса через матрицы процесса и броуновского движения $G$ и $Q$:\n",
        "\n",
        "$$ d\\phi = \\sum_{i=1}\\frac{\\partial \\phi}{\\partial x_{i}}f(x(t),t)_{i}dt + \\sum_{i=1}\\frac{\\partial \\phi}{\\partial x_{i}}[G(x(t),t)dw(t)]_{i} + \\frac{1}{2}\\sum_{i,j}\\frac{\\partial^{2} \\phi}{\\partial x_{i}\\partial x_{j}}[GQG^{T}]_{ij}dt.$$\n",
        "\n",
        "3. Домножаем все на $dt$ и берем математическое ожидание:\n",
        "\n",
        "$$ \\frac{d\\mathbb{E}\\phi}{dt} = \\sum_{i=1}\\mathbb{E}[\\frac{\\partial \\phi}{\\partial x_{i}}f(x(t),t)]_{i} + 0 +\n",
        "\\frac{1}{2}\\sum_{i,j}\\mathbb{E}[\\frac{\\partial^{2} \\phi}{\\partial x_{i}\\partial x_{j}}[GQG^{T}]_{ij}].$$\n",
        "\n",
        "4. Дважды применяем формулу интегрирования по частям и получаем\n",
        "\n",
        "$$ \\frac{\\partial p(x(t),t)}{\\partial t} + \\sum_{i}\\frac{\\partial}{\\partial x_{i}}[f_{i}(x(t),t)p(x(t),t)]=\n",
        "\\frac{1}{2}\\sum_{i,j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}} [[GQG^{T}]_{ij}p(x(t),t)] .$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddebe722",
      "metadata": {
        "id": "ddebe722"
      },
      "source": [
        "$\\textbf{Вопросы в аудиторию:}$ почему зануляется математическое ожидание в пункте 3?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4267f804",
      "metadata": {
        "id": "4267f804"
      },
      "source": [
        "#### 1.3.4. Процессс Орнштейна—Уленбека"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Ornstein-Uhlenbeck-5traces.svg/1200px-Ornstein-Uhlenbeck-5traces.svg.png)"
      ],
      "metadata": {
        "id": "l8xdP-aelUPh"
      },
      "id": "l8xdP-aelUPh"
    },
    {
      "cell_type": "markdown",
      "id": "e46bb1d3",
      "metadata": {
        "id": "e46bb1d3"
      },
      "source": [
        "Снова вспомним мотивацию:\n",
        "    \n",
        "- Знать $\\textbf{истинное}$ значение для $\\nabla_{x}\\log q(x(t)|x(0))$.\n",
        "- На него обучать score-модель $s_{\\theta}(x,t)$.\n",
        "- Запускать обратное СДУ для генерации изображений.\n",
        "\n",
        "Казалось бы, мы получили способ нахождения нужной плотности через решение уравнения Фоккера—Планка, но проблема состоит в том $\\textbf{решать уравнение в частных производных сложно}$, а значит, нужно найти путь полегче.\n",
        "\n",
        "$\\textbf{Идея}$\n",
        "\n",
        "1. Расссмотрим такой процесс, у которого маргинальное распределение всегда нормальное.\n",
        "2. Найдем два первых момента распределений.\n",
        "\n",
        "К счастью, такой процесс существует — процесс Орнштейна—Уленбека, дрифт которого является $\\textbf{линейной}$ функцией по аргументу. И это ровно та причина, по которой многие годы ученые рассматривали диффузионные модели, в которых дрифт был линейной функцией, а не обучаемой нейросетью. Потому что маргинал $ \\log q(x(t)|x(0))$ надо знать.\n",
        "\n",
        "Пример процесса Орнштейна—Уленбека:\n",
        "\n",
        "$$ dx(t) = \\alpha x(t) dt + G(x(t),t)dw(t) .$$\n",
        "\n",
        "Больше информации о свойствах этого процесса можно узнать по ссылке\n",
        "\n",
        "https://www.maths.ed.ac.uk/~toh/Files/hypercontractivity.pdf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0f036a",
      "metadata": {
        "id": "be0f036a"
      },
      "source": [
        "#### 1.3.5. Обыкновенные дифференциальные уравнения для нахождения моментов"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://iimg.su/s/16/ol01PKmT5UOUQA6xnBAenBkrpwNka1L3LFVGL4Pz.png)"
      ],
      "metadata": {
        "id": "4j50oCrTwAPg"
      },
      "id": "4j50oCrTwAPg"
    },
    {
      "cell_type": "markdown",
      "id": "4b9324b5",
      "metadata": {
        "id": "4b9324b5"
      },
      "source": [
        "Мы подошли к этапу, когда нам известен закон каждого искомого маргинального распределения, но мы не знаем его моменты:\n",
        "\n",
        "$$ q(x(t)|x(0)) = \\mathcal{N}(x(t)| \\mu(t) = ? , \\Sigma(t) = ?). $$\n",
        "\n",
        "Поиском $ \\mu(t), \\Sigma(t)$ мы сейчас и займемся."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d55ccf5",
      "metadata": {
        "id": "8d55ccf5"
      },
      "source": [
        "$\\textbf{Моменты маргинальных распределений}$\n",
        "\n",
        "\n",
        "Выбираем линейный дрифт: $f(x_{t},t) = \\alpha x_{t}$.\n",
        "\n",
        "Выпишем Формулу Ито для такого дрифта:\n",
        "\n",
        "$$ \\frac{\\mathbb{E}f(x_{t},t)}{dt} = \\alpha \\mathbb{E}f(x_{t},t).$$\n",
        "\n",
        "Если обозначить среднее процесса через $m(t) =\\mathbb{E}f(x_{t},t)$, то на среднее процесса получаем следующее стохастическое дифференциальное уравнение (обыкновенное дифференциальное уравнение Эйлера):\n",
        "\n",
        "$$ \\frac{dm(t)}{dt} = \\alpha m(t).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cce221a",
      "metadata": {
        "id": "8cce221a"
      },
      "source": [
        "$\\textbf{Задание 1.}$ Определите моменты маргинальных распределений процесса Орнштейна—Улебенбека:\n",
        "$$ dx = -\\lambda x dt + d\\beta, \\quad x(0)=x_{0},$$\n",
        "$\\lambda > 0, \\beta(t)$ — броуновское движение с диффузионной константой $q$.\n",
        "\n",
        "$\\textbf{Задание 2.}$ Определите моменты маргинальных распределений синусно-диффузионного процесса:\n",
        "$$ dx = \\sin(x)dt + d\\beta(t)$$  с диффузионной константой $q$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbb5343",
      "metadata": {
        "id": "3bbb5343"
      },
      "source": [
        "Маргинальные рапсределения для нашего прямого процесса будут выглядеть так:\n",
        "\n",
        "\n",
        "$$p_{0t}(x(t)|x(0)) = \\mathcal{N}(x(t); e^{-\\frac{1}{4}t^{2} (\\overline{\\beta_{max}} -\\overline{\\beta_{min}})  - \\frac{1}{2}t\\overline{\\beta_{min}}}x(0),I - Ie^{-\\frac{1}{2}t^{2}(\\overline{\\beta_{max}} - \\overline{\\beta_{min}})  - t\\overline{\\beta_{min}}} ).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ChessUrl]( https://stable-diffusion-art.com/wp-content/uploads/2022/12/image-79.png \"chess\")   "
      ],
      "metadata": {
        "id": "5Bj1TbZBPJj4"
      },
      "id": "5Bj1TbZBPJj4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19aeaa0b",
      "metadata": {
        "id": "19aeaa0b"
      },
      "outputs": [],
      "source": [
        "class VP_SDE:\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Construct a Variance Preserving SDE.\n",
        "\n",
        "        Args:\n",
        "          beta_min: value of beta(0)\n",
        "          beta_max: value of beta(1)\n",
        "          N: number of discretization steps\n",
        "        \"\"\"\n",
        "        self.N = config.sde.N\n",
        "        self.beta_0 = config.sde.beta_min\n",
        "        self.beta_1 = config.sde.beta_max\n",
        "        self._T = config.sde.T\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        \"\"\"\n",
        "        returns: terminal time [1]\n",
        "        \"\"\"\n",
        "        return self._T\n",
        "\n",
        "\n",
        "    def sde(self, x, t):\n",
        "        \"\"\"\n",
        "        Calculate drift coeff. and diffusion coeff. in forward SDE\n",
        "\n",
        "        input:\n",
        "        - x\n",
        "        - t\n",
        "\n",
        "        returns: drift with size [B,C,H,W], diffuison with size [1]\n",
        "        \"\"\"\n",
        "        beta_t = self.beta_0 + (self.beta_1 - self.beta_0) * t # linear law\n",
        "        drift = -0.5 * beta_t[:, None, None, None] * x # [B,C,H,W]\n",
        "        diffusion = torch.sqrt(beta_t) #[1]\n",
        "\n",
        "        return drift, diffusion\n",
        "\n",
        "\n",
        "    def marginal_prob(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Calculate marginal q(x_t|x_0)'s mean and std\n",
        "\n",
        "        input:\n",
        "        - x\n",
        "        - t\n",
        "\n",
        "        returns:\n",
        "        \"\"\"\n",
        "        log_mean = - 0.5 * t * self.beta_0 - 0.25 * (t ** 2) * (self.beta_1 - self.beta_0)  # ??\n",
        "        mean = torch.exp(log_mean[:, None, None, None]) * x_0 # [B,C,H,W]\n",
        "        std = torch.sqrt(1 - torch.exp(log_mean * 2)) # ??\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "    def marginal_std(self, t):\n",
        "        \"\"\"\n",
        "        Calculate marginal q(x_t|x_0)'s std\n",
        "\n",
        "        input:\n",
        "        - x\n",
        "        - t\n",
        "\n",
        "        returns:\n",
        "        \"\"\"\n",
        "        log_mean = - 0.5 * t * self.beta_0 - 0.25 * (t ** 2) * (self.beta_1 - self.beta_0)\n",
        "        std = torch.sqrt(1 - torch.exp(log_mean * 2))\n",
        "\n",
        "        return std\n",
        "\n",
        "\n",
        "    def prior_sampling(self, shape):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        return torch.randn(*shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29de92c",
      "metadata": {
        "id": "f29de92c"
      },
      "source": [
        "### 1.4. Обратный диффузионный процесс"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://theacademic.com/wp-content/uploads/2023/09/reverse_diffusion.png)"
      ],
      "metadata": {
        "id": "anLTChfl_Vhk"
      },
      "id": "anLTChfl_Vhk"
    },
    {
      "cell_type": "markdown",
      "id": "685ab486",
      "metadata": {
        "id": "685ab486"
      },
      "source": [
        "Следующий класс как раз и определяет семплирование обратным диффузионным процессом. Здесь значение истинного $\\nabla_{x}\\log p_{t}(x)$ заменяется значением обученной нами score-функции $s_{\\theta}^{*}(x,t)$:\n",
        "    \n",
        "$$ dx =  [f(x,t) - \\frac{1}{2}G(t)G(t)^{T}s_{\\theta}^{*}(x,t)]dt + G(t)dw_{t}. $$\n",
        "\n",
        "Тогда итерационная схема обратного процесса выглядит так:\n",
        "\n",
        "$$ x_{i} = x_{i+1} - f_{i+1}(x_{i+1}) + G_{i+1}G_{i+1}^{T}s^{*}_{\\theta}(x_{i+1},i+1) + G_{i+1}z_{i+1} .$$\n",
        "\n",
        "$\\textbf{Вопрос к аудитории}:$\n",
        "что представляет собой $z_{i+1} $?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febb9dcd",
      "metadata": {
        "id": "febb9dcd"
      },
      "outputs": [],
      "source": [
        "class RSDE:\n",
        "\n",
        "    def __init__(self, vp_sde, ode_sampling):\n",
        "        self.N = vp_sde.N\n",
        "        self.ode_sampling = ode_sampling\n",
        "        self.sde_fn = vp_sde.sde\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        return vp_sde._T\n",
        "\n",
        "\n",
        "    def sde(self, x, t, score_fn, y=None):\n",
        "        \"\"\"\n",
        "        Create the drift and diffusion functions for the reverse SDE/ODE.\n",
        "\n",
        "        y is here for class-conditional generation through score SDE/ODE\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Calculate drift and diffusion for reverse SDE/ODE\n",
        "\n",
        "\n",
        "        ode_sampling - True -> reverse ODE\n",
        "        ode_sampling - False -> reverse SDE\n",
        "        \"\"\"\n",
        "        drift, diffusion = self.sde_fn(x, t)\n",
        "        score = score_fn(x, t) # получаем значение score-функции\n",
        "\n",
        "        # (-1/2 beta_t * x_t - beta_t * score)\n",
        "        drift = drift - diffusion[:, None, None, None] ** 2 * score\n",
        "        return drift, diffusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904f22fe",
      "metadata": {
        "id": "904f22fe"
      },
      "source": [
        "### 1.5. Численная схема решения ОДУ (СДУ)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://drive.google.com/uc?id=1CGFbtY2mCjlIY8pjvoGevfa_32d4b1dj)"
      ],
      "metadata": {
        "id": "bWznsgk_BZ3q"
      },
      "id": "bWznsgk_BZ3q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь вы умеете запускать СДУ в обратном времени. Разумеется, тут не все так просто, как с ОДУ, где мы просто интегрируем в обратном направлении то же уравнение. Отсюда следует логичный вопрос.\n",
        "\n",
        "$\\textbf{Вопрос для размышления:}$\n",
        "А нет ли такого ОДУ, которое ведет себя, как наше СДУ?\n",
        "\n",
        "То есть нет ли какого-то ОДУ, чьи фазовые траектории создают такие же маргинальные распределения, что и случайные траектории СДУ?\n",
        "\n",
        "$\\textbf{Вопросы:}$\n",
        "- Зачем нужно ОДУ?\n",
        "- Что бы мы хотели от такого СДУ?\n",
        "- Как можно было бы прийти к идее создания такого СДУ?\n",
        "\n",
        "Таким образом, если ОДУ имеет те же маргианльные распределения процесса, что и СДУ, то можно запускать ОДУ в прямом направлении, а потом просто интегрировать в обратном направлении, тем самым имитируя обратный процесс.\n",
        "\n",
        "Займемся выводом такого ОДУ.\n",
        "\n",
        "Снова рассмотрим наше СДУ для прямого процесса, чтобы вывести ОДУ, которое имеет те же маргиналы, что и это СДУ:\n",
        "\n",
        "$$dx_{t} = f(x_{t},t)dt + G(x_{t},t)dw_{t} .$$\n",
        "\n",
        "$\\textbf{Вопросы:}$\n",
        "- Дрифт — скалярная функция?\n",
        "- Что такое матрица процесса?\n",
        "\n",
        "Тогда запишем уравнение Фоккера—Планка:\n",
        "\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} [f_{i}(x,t)p_{t}(x)] + \\frac{1}{2}\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\frac{\\partial^{2}}{\\partial x_{i} \\partial x_{j}}[\\sum_{k=1}^{d}G_{ik}G_{jk}p_{t}(x)].$$\n",
        "\n",
        "Перепишем данное уравнение:\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} [f_{i}(x,t)p_{t}(x)] + \\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial x_{i} }[\\sum_{j=1}^{d}\\frac{\\partial}{\\partial x_{j} }[\\sum_{k=1}^{d}G_{ik}G_{jk}p_{t}(x)]] $$\n",
        "\n",
        "Распишем последнюю производную последнего выражения:\n",
        "\n",
        "$$ \\sum_{j=1}^{d}\\frac{\\partial}{\\partial x_{j} }[\\sum_{k=1}^{d}G_{ik}G_{jk}p_{t}(x)] = \\sum_{j=1}^{d}\\frac{\\partial}{\\partial x_{j} }[\\sum_{k=1}^{d}G_{ik}G_{jk}]p_{t}(x) + \\sum_{j=1}^{d}\\sum_{k=1}^{d}G_{ik}G_{jk}[\\frac{\\partial}{\\partial x_{j}}\\log p_{t}(x)] = $$\n",
        "\n",
        "$$ = p_{t}(x)\\nabla_{x}[G(x,t)G(x,t)^{T}] + p_{t}(x)G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x) $$\n",
        "\n",
        "Тогда в исходное уравнение Фоккера—Планка подставим итоговое выражение для посчитанной второй производной:\n",
        "\n",
        "\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} [f_{i}(x,t)p_{t}(x)] +  \\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial x_{i}} [p_{t}(x)\\nabla_{x}[G(x,t)G(x,t)^{T}] + p_{t}(x)G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x)].$$\n",
        "\n",
        "Объединим два слагаемых в правой части в одно:\n",
        "\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} \\{ f_{i}(x,t)p_{t}(x) - \\frac{1}{2}[\\nabla_{x}[G(x,t)G(x,t)^{T}] +  G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x)]p_{t}(x)\\}  = -\\sum_{i=1}^{d}\\frac{\\partial}{\\partial x_{i}}[\\hat{f}_{i}(x,t)p_{t}(x)],$$\n",
        "\n",
        "где через дрифт с шляпкой мы обозначили\n",
        "\n",
        "$$\\hat{f}(x,t) = f(x,t) - \\frac{1}{2}\\nabla_{x}[G(x,t)G(x,t)^{T}] - \\frac{1}{2}G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x).$$\n",
        "\n",
        "$\\textbf{Вопрос к аудитории}:$\n",
        "почему выше не описан второй терм из последней формулы в уравнении обратного диффузионного процесса?\n",
        "\n",
        "Таким образом,  ОДУ $dx = \\hat{f}(x,t)dt$ имеет те же маргинальные траектории, что и СДУ $dx = f(x,t)dt + G(x,t)dw_{t}$.\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "чем различаются траектории ОДУ и СДУ?\n",
        "\n",
        "Это ОДУ называется в литературе Probablility flow."
      ],
      "metadata": {
        "id": "mb9gv9SjBlxr"
      },
      "id": "mb9gv9SjBlxr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc5c328",
      "metadata": {
        "id": "ffc5c328"
      },
      "outputs": [],
      "source": [
        "class EulerDiffEqSolver:\n",
        "    def __init__(self, sde, rsde, score_fn, ode_sampling = False):\n",
        "        self.sde = sde\n",
        "        self.score_fn = score_fn\n",
        "        self.ode_sampling = ode_sampling\n",
        "        self.rsde =  rsde\n",
        "\n",
        "    def step(self, x, t, y=None):\n",
        "        \"\"\"\n",
        "        Implement reverse SDE/ODE Euler solver\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        x_mean = deterministic part\n",
        "        x = x_mean + noise (yet another noise sampling)\n",
        "        \"\"\"\n",
        "\n",
        "        dt = -1 / self.rsde.N\n",
        "        z = torch.randn(x.shape).to(x.device)\n",
        "\n",
        "        drift, diffusion = self.rsde.sde(x, t, self.score_fn)\n",
        "        x_mean = x + drift * dt\n",
        "        x = x_mean + np.sqrt(-dt) * diffusion[:, None, None, None] * z\n",
        "\n",
        "        return x, x_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6. Оценка правдоподобия диффузионной модели"
      ],
      "metadata": {
        "id": "KbMvtM75JJAr"
      },
      "id": "KbMvtM75JJAr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/normalizing_flow_layout.png)"
      ],
      "metadata": {
        "id": "Q7pBOxTwL-UO"
      },
      "id": "Q7pBOxTwL-UO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Остался один из главных вопросов: как нам оценить качество генерации диффузионных моделей?\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "какие метрики качества вы бы предложили?\n",
        "\n",
        "Можно оценить правдоподобие.\n",
        "\n",
        "У нас есть ОДУ обратного процесса:\n",
        "\n",
        "$$dx = \\{ f(x,t) - \\frac{1}{2}\\nabla_{x}[G(x,t)G(x,t)^{T} - \\frac{1}{2}G(x,t)G(x,t)^{T}s_{\\theta}(x,t)\\}dt,$$\n",
        "\n",
        "$$ dx = \\hat{f}(x,t)dt.$$\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "ОДУ прямого процесса отличается от обратного ОДУ?\n",
        "\n",
        "Согласно формуле замены переменной в кратном интеграле\n",
        "$$\\log p_{0}(x(0)) = \\log p_{T}(x(T)) + \\int_{0}^{T} \\nabla_{x}\\hat{f}(x,t)dt.$$\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "оцениваем мы правдоподобие на инференсе или на трейне?\n",
        "\n",
        "\n",
        "Больше про нормализационные потоки и детерминистичные функции преобразования можно посмотреть здесь:\n",
        " https://pytorch-lighting.readthedocs.io/en/latest/notebooks/course_UvA-DL/09-normalizing-flows.html"
      ],
      "metadata": {
        "id": "lJG2xPlXJRQq"
      },
      "id": "lJG2xPlXJRQq"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CpRBiokB2kA2"
      },
      "id": "CpRBiokB2kA2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHnqiZp82krW"
      },
      "source": [
        "## 2.  Прямой диффузионный процесс"
      ],
      "id": "zHnqiZp82krW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ChessUrl]( https://stable-diffusion-art.com/wp-content/uploads/2022/12/image-79.png \"chess\")   \n",
        "\n",
        "Маргинальные распределения для нашего прямого процесса будут выглядеть так:\n",
        "\n",
        "\n",
        "$$p_{0t}(x(t)|x(0)) = \\mathcal{N}(x(t); e^{-\\frac{1}{4}t^{2} (\\overline{\\beta_{max}} -\\overline{\\beta_{min}})  - \\frac{1}{2}t\\overline{\\beta_{min}}}x(0),I - Ie^{-\\frac{1}{2}t^{2}(\\overline{\\beta_{max}} - \\overline{\\beta_{min}})  - t\\overline{\\beta_{min}}} ).$$"
      ],
      "metadata": {
        "id": "P5TbvNrZX3Kb"
      },
      "id": "P5TbvNrZX3Kb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssgpghrK2krW"
      },
      "outputs": [],
      "source": [
        "class VP_SDE:\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Construct a Variance Preserving SDE.\n",
        "\n",
        "        Args:\n",
        "          beta_min: value of beta(0)\n",
        "          beta_max: value of beta(1)\n",
        "          N: number of discretization steps\n",
        "        \"\"\"\n",
        "        self.N = config.sde.N\n",
        "        self.beta_0 = config.sde.beta_min\n",
        "        self.beta_1 = config.sde.beta_max\n",
        "        self._T = config.sde.T\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        \"\"\"\n",
        "        returns: terminal time [1]\n",
        "        \"\"\"\n",
        "        return self._T\n",
        "\n",
        "\n",
        "    def sde(self, x, t):\n",
        "        \"\"\"\n",
        "        Calculate drift coeff. and diffusion coeff. in forward SDE\n",
        "\n",
        "        input:\n",
        "        - x\n",
        "        - t\n",
        "\n",
        "        returns: drift with size [B,C,H,W], diffuison with size [1]\n",
        "        \"\"\"\n",
        "        beta_t = self.beta_0 + (self.beta_1 - self.beta_0) * t # linear law\n",
        "        drift = -0.5 * beta_t[:, None, None, None] * x # [B,C,H,W]\n",
        "        diffusion = torch.sqrt(beta_t) #[1]\n",
        "\n",
        "        return drift, diffusion\n",
        "\n",
        "\n",
        "    def marginal_prob(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Calculate marginal q(x_t|x_0)'s mean and std\n",
        "\n",
        "        input:\n",
        "        - x\n",
        "        - t\n",
        "\n",
        "        returns:\n",
        "        \"\"\"\n",
        "        log_mean = - 0.5 * t * self.beta_0 - 0.25 * (t ** 2) * (self.beta_1 - self.beta_0)  # ??\n",
        "        mean = torch.exp(log_mean[:, None, None, None]) * x_0 # [B,C,H,W]\n",
        "        std = torch.sqrt(1 - torch.exp(log_mean * 2)) # ??\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "    def marginal_std(self, t):\n",
        "        \"\"\"\n",
        "        Calculate marginal q(x_t|x_0)'s std\n",
        "\n",
        "        input:\n",
        "        - x\n",
        "        - t\n",
        "\n",
        "        returns:\n",
        "        \"\"\"\n",
        "        log_mean = - 0.5 * t * self.beta_0 - 0.25 * (t ** 2) * (self.beta_1 - self.beta_0)\n",
        "        std = torch.sqrt(1 - torch.exp(log_mean * 2))\n",
        "\n",
        "        return std\n",
        "\n",
        "\n",
        "    def prior_sampling(self, shape):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        return torch.randn(*shape)"
      ],
      "id": "ssgpghrK2krW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Обратный диффузионный процесс"
      ],
      "metadata": {
        "id": "CtcdCCJpYCTz"
      },
      "id": "CtcdCCJpYCTz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://theacademic.com/wp-content/uploads/2023/09/reverse_diffusion.png)"
      ],
      "metadata": {
        "id": "xh4z46ZZ2krX"
      },
      "id": "xh4z46ZZ2krX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaW76vNu2krX"
      },
      "source": [
        "Следующий класс как раз и определяет семплирование обратным диффузионным процессом. Здесь значение истинного $\\nabla_{x}\\log p_{t}(x)$ заменяется значением обученной нами score-функции $s_{\\theta}^{*}(x,t)$:\n",
        "    \n",
        "$$ dx =  [f(x,t) - \\frac{1}{2}G(t)G(t)^{T}s_{\\theta}^{*}(x,t)]dt + G(t)dw_{t} .$$\n",
        "\n",
        "Тогда итерационная схема обратного процесса выглядит так:\n",
        "\n",
        "$$ x_{i} = x_{i+1} - f_{i+1}(x_{i+1}) + G_{i+1}G_{i+1}^{T}s^{*}_{\\theta}(x_{i+1},i+1) + G_{i+1}z_{i+1} .$$\n",
        "\n",
        "\n"
      ],
      "id": "DaW76vNu2krX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edVoqn1f2krX"
      },
      "outputs": [],
      "source": [
        "class RSDE:\n",
        "\n",
        "    def __init__(self, vp_sde, ode_sampling):\n",
        "        self.N = vp_sde.N\n",
        "        self.ode_sampling = ode_sampling\n",
        "        self.sde_fn = vp_sde.sde\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        return vp_sde._T\n",
        "\n",
        "\n",
        "    def sde(self, x, t, score_fn, y=None):\n",
        "        \"\"\"\n",
        "        Create the drift and diffusion functions for the reverse SDE/ODE.\n",
        "\n",
        "        y is here for class-conditional generation through score SDE/ODE\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Calculate drift and diffusion for reverse SDE/ODE\n",
        "\n",
        "\n",
        "        ode_sampling - True -> reverse ODE\n",
        "        ode_sampling - False -> reverse SDE\n",
        "        \"\"\"\n",
        "        drift, diffusion = self.sde_fn(x, t)\n",
        "        score = score_fn(x, t) # получаем значение score-функции\n",
        "\n",
        "        # (-1/2 beta_t * x_t - beta_t * score)\n",
        "        drift = drift - diffusion[:, None, None, None] ** 2 * score\n",
        "        return drift, diffusion\n"
      ],
      "id": "edVoqn1f2krX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8uihoiI2krX"
      },
      "source": [
        "## 4. Численная схема решения ОДУ (СДУ)"
      ],
      "id": "M8uihoiI2krX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://drive.google.com/uc?id=1CGFbtY2mCjlIY8pjvoGevfa_32d4b1dj)"
      ],
      "metadata": {
        "id": "9Z9ynnj72krY"
      },
      "id": "9Z9ynnj72krY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь вы умеете запускать СДУ в обратном времени. Разумеется, тут не все так просто, как с ОДУ, где мы просто интегрируем в обратном направлении то же уравнение. Отсюда логичный вопрос:\n",
        "\n",
        "$\\textbf{Вопрос для размышления:}$\n",
        "Нет ли такого ОДУ, который ведет себя, как наше СДУ?\n",
        "\n",
        "То есть нет ли какого-то ОДУ, чьи фазовые траектории создают такие же маргинальные распределения, что и случайные траектории СДУ?\n",
        "\n",
        "$\\textbf{Вопросы:}$\n",
        "- Зачем нужно ОДУ?\n",
        "- Что бы мы хотели от такого СДУ?\n",
        "- Как можно было бы прийти к идее создания такого СДУ?\n",
        "\n",
        "Таким образом, если ОДУ имеет те же маргинальные распределения процесса, что и СДУ, то можно просто запускать ОДУ в прямом направлении, а потом интегрировать в обратном направлении, тем самым имитируя обратный процесс.\n",
        "\n",
        "Займемся выводом такого ОДУ.\n",
        "\n",
        "Снова рассмотрим наше СДУ для прямого процесса, чтобы вывести ОДУ, которое имеет те же маргиналы, что и это СДУ:\n",
        "\n",
        "$$dx_{t} = f(x_{t},t)dt + G(x_{t},t)dw_{t} $$\n",
        "\n",
        "$\\textbf{Вопросы:}$\n",
        "- Дрифт — скалярная функция?\n",
        "- Что такое матрица процесса?\n",
        "\n",
        "Тогда запишем уравнение Фоккера—Планка:\n",
        "\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} [f_{i}(x,t)p_{t}(x)] + \\frac{1}{2}\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\frac{\\partial^{2}}{\\partial x_{i} \\partial x_{j}}[\\sum_{k=1}^{d}G_{ik}G_{jk}p_{t}(x)]$$\n",
        "\n",
        "Перепишем данное уравнение:\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} [f_{i}(x,t)p_{t}(x)] + \\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial x_{i} }[\\sum_{j=1}^{d}\\frac{\\partial}{\\partial x_{j} }[\\sum_{k=1}^{d}G_{ik}G_{jk}p_{t}(x)]] $$\n",
        "\n",
        "Распишем последнюю производную последнего выражения:\n",
        "\n",
        "$$ \\sum_{j=1}^{d}\\frac{\\partial}{\\partial x_{j} }[\\sum_{k=1}^{d}G_{ik}G_{jk}p_{t}(x)] = \\sum_{j=1}^{d}\\frac{\\partial}{\\partial x_{j} }[\\sum_{k=1}^{d}G_{ik}G_{jk}]p_{t}(x) + \\sum_{j=1}^{d}\\sum_{k=1}^{d}G_{ik}G_{jk}[\\frac{\\partial}{\\partial x_{j}}\\log p_{t}(x)] = $$\n",
        "\n",
        "$$ = p_{t}(x)\\nabla_{x}[G(x,t)G(x,t)^{T}] + p_{t}(x)G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x) $$\n",
        "\n",
        "Тогда в исходного Фоккера—Планка подставим итоговое выражение для посчитанной второй производной:\n",
        "\n",
        "\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} [f_{i}(x,t)p_{t}(x)] +  \\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial x_{i}} [p_{t}(x)\\nabla_{x}[G(x,t)G(x,t)^{T}] + p_{t}(x)G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x)]$$\n",
        "\n",
        "Объединим два слагаемых в правой части в одно:\n",
        "\n",
        "$$\\frac{\\partial p_{t}(x)}{\\partial t} = - \\sum_{i=1}^{d} \\frac{\\partial}{\\partial x_{i}} \\{ f_{i}(x,t)p_{t}(x) - \\frac{1}{2}[\\nabla_{x}[G(x,t)G(x,t)^{T}] +  G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x)]p_{t}(x)\\}  = -\\sum_{i=1}^{d}\\frac{\\partial}{\\partial x_{i}}[\\hat{f}_{i}(x,t)p_{t}(x)]$$\n",
        "\n",
        "Здесь через дрифт с шляпкой мы обозначили функцию\n",
        "\n",
        "$$\\hat{f}(x,t) = f(x,t) - \\frac{1}{2}\\nabla_{x}[G(x,t)G(x,t)^{T}] - \\frac{1}{2}G(x,t)G(x,t)^{T}\\nabla_{x}\\log p_{t}(x)$$\n",
        "\n",
        "$\\textbf{Вопрос к аудитории.}$\n",
        "Почему выше не описан второй терм из последней формулы в уравнении обратного диффузионного процесса?\n",
        "\n",
        "Таким образом,  ОДУ $dx = \\hat{f}(x,t)dt$ имеет те же маргинальные траектории, что и СДУ $dx = f(x,t)dt + G(x,t)dw_{t}$.\n",
        "\n",
        "$\\textbf{Вопрос:}$\n",
        "чем различаются траектории ОДУ и СДУ?\n",
        "\n",
        "Это ОДУ называется в литературе Probablility flow."
      ],
      "metadata": {
        "id": "22_cCw5U2krY"
      },
      "id": "22_cCw5U2krY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ievlInaZ2krY"
      },
      "outputs": [],
      "source": [
        "class EulerDiffEqSolver:\n",
        "    def __init__(self, sde, rsde, score_fn, ode_sampling = False):\n",
        "        self.sde = sde\n",
        "        self.score_fn = score_fn\n",
        "        self.ode_sampling = ode_sampling\n",
        "        self.rsde =  rsde\n",
        "\n",
        "    def step(self, x, t, y=None):\n",
        "        \"\"\"\n",
        "        Implement reverse SDE/ODE Euler solver\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        x_mean = deterministic part\n",
        "        x = x_mean + noise (yet another noise sampling)\n",
        "        \"\"\"\n",
        "\n",
        "        dt = -1 / self.rsde.N\n",
        "        z = torch.randn(x.shape).to(x.device)\n",
        "\n",
        "        drift, diffusion = self.rsde.sde(x, t, self.score_fn)\n",
        "        x_mean = x + drift * dt\n",
        "        x = x_mean + np.sqrt(-dt) * diffusion[:, None, None, None] * z\n",
        "\n",
        "        return x, x_mean"
      ],
      "id": "ievlInaZ2krY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8dbb296"
      },
      "source": [
        "## 5. Модель глубокого обучения для DDPM"
      ],
      "id": "b8dbb296"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b7189e6"
      },
      "source": [
        "Эта часть представляет собой описание и имплементацию деталей модели глубокого обучения для работы с изображениями, а также некоторых оптимизационных инструментов, обеспечивающих значимый прирост качества на практике."
      ],
      "id": "7b7189e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2fcbfaa"
      },
      "source": [
        "### 5.1. Экспоненциальное скользящее среднее"
      ],
      "id": "f2fcbfaa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda9a63e"
      },
      "source": [
        "Источники:\n",
        "- https://jcsustem.com/blog/benefits-of-using-ema-in-deep-learning\n",
        "\n",
        "- https://shallbd.com/understanding-ema-in-machine-learning-everything-you-need-to-know/\n",
        "\n",
        "Что такое экспоненциальное скользящее среднее (ЕМА)?\n",
        "\n",
        "EMA позволяет обновлять веса модели глубокого обучения путем усреднения предыдущих значений, придавая большую важность последним. Применение данного инструмента для моделей несет определенную значимость, поскольку, давая большую значимость последним значениям весов, ЕМА может охватить тренд оптимизации весов, делая модель более робастной.\n",
        "\n",
        "ЕМА работает следующим образом:\n",
        "\n",
        "$$ \\theta_{ema} = \\lambda\\theta_{current} + (1-\\lambda)\\theta_{ema} .$$\n",
        "\n",
        "Преимущества использования ЕМА:\n",
        "    \n",
        "- 1. Стабилизирует модель обучения за счет сглаживания колебания градиентов.\n",
        "- 2. Уменьшает шум при обновлении весов модели.\n",
        "- 3. Обеспечивает ускорение сходимости.\n",
        "- 4. Обеспечивает большую устойчивость модели по отношению к гиперпараметрам.\n"
      ],
      "id": "fda9a63e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de0a27f5"
      },
      "source": [
        "Ниже преставлен класс для ЕМА.\n",
        "    \n",
        "1.  Инициализация\n",
        "\n",
        "Важно понимать, что мы определяем фактор сглаживания колебаний градиентов модели, а также инициализируем ЕМА параметрами модели (то есть текущим скользящим средним). Такое текущее скользящее среднеее мы сохраняем в переменную **shadow_params**.\n",
        "\n",
        "2. Копирование\n",
        "\n",
        "Эта операция позволяет инициализировать параметры модели при помощи текущего скользящего среднего.\n",
        "\n",
        "3. Сохранение\n",
        "\n",
        "Следующий метод позволяет нам сохранять все текущие параметры модели.\n",
        "\n",
        "4. Словарь (загрузка/разгрузка)\n",
        "\n",
        "Позволяет сохранять текущий фактор сглаживания и текущее скользящее среднее.\n",
        "\n",
        "5. Обновление\n",
        "\n",
        "Мы модифицируем переменную **shadow_params**, в которой хранятся значения текущего скользящего среднего, и обновляем его с учетом прошлого скользящего среднего и крайних весов модели."
      ],
      "id": "de0a27f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1652ccee"
      },
      "outputs": [],
      "source": [
        "class ExponentialMovingAverage:\n",
        "\n",
        "    \"\"\"\n",
        "    Maintains (exponential) moving average of a set of parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parameters, decay, use_num_updates=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          parameters: Iterable of `torch.nn.Parameter`; usually the result of\n",
        "            `model.parameters()`.\n",
        "          decay: The exponential decay.\n",
        "          use_num_updates: Whether to use number of updates when computing\n",
        "            averages.\n",
        "        \"\"\"\n",
        "        if decay < 0.0 or decay > 1.0:\n",
        "            raise ValueError('Decay must be between 0 and 1')\n",
        "        self.decay = decay\n",
        "        self.num_updates = 0 if use_num_updates else None\n",
        "        self.shadow_params = [p.clone().detach()\n",
        "                              for p in parameters if p.requires_grad]\n",
        "        self.collected_params = []\n",
        "\n",
        "    def copy_to(self, parameters):\n",
        "        \"\"\"\n",
        "        Copy current parameters into given collection of parameters.\n",
        "\n",
        "        Args:\n",
        "        parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
        "        updated with the stored moving averages.\n",
        "        \"\"\"\n",
        "        parameters = [p for p in parameters if p.requires_grad]\n",
        "        for s_param, param in zip(self.shadow_params, parameters):\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(s_param.data)\n",
        "\n",
        "    def store(self, parameters):\n",
        "        \"\"\"\n",
        "        Save the current parameters for restoring later.\n",
        "\n",
        "        Args:\n",
        "          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
        "            temporarily stored.\n",
        "        \"\"\"\n",
        "        self.collected_params = [param.clone() for param in parameters]\n",
        "\n",
        "\n",
        "    def state_dict(self):\n",
        "        return dict(decay=self.decay, num_updates=self.num_updates,\n",
        "                    shadow_params=self.shadow_params)\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.decay = state_dict['decay']\n",
        "        self.num_updates = state_dict['num_updates']\n",
        "        self.shadow_params = state_dict['shadow_params']\n",
        "\n",
        "\n",
        "    def update(self, parameters):\n",
        "        \"\"\"\n",
        "        Update currently maintained parameters.\n",
        "\n",
        "        Call this every time the parameters are updated, such as the result of\n",
        "        the `optimizer.step()` call.\n",
        "\n",
        "        Args:\n",
        "          parameters: Iterable of `torch.nn.Parameter`; usually the same set of\n",
        "            parameters used to initialize this object.\n",
        "        \"\"\"\n",
        "        decay = self.decay\n",
        "        if self.num_updates is not None:\n",
        "            self.num_updates += 1\n",
        "            decay = min(decay, (1 + self.num_updates) / (10 + self.num_updates))\n",
        "        one_minus_decay = 1.0 - decay\n",
        "        with torch.no_grad():\n",
        "            parameters = [p for p in parameters if p.requires_grad]\n",
        "            for s_param, param in zip(self.shadow_params, parameters):\n",
        "                s_param.sub_(one_minus_decay * (s_param - param))\n",
        "\n",
        "\n",
        "    def restore(self, parameters):\n",
        "        \"\"\"\n",
        "        Restore the parameters stored with the `store` method.\n",
        "        Useful to validate the model with EMA parameters without affecting the\n",
        "        original optimization process. Store the parameters before the\n",
        "        `copy_to` method. After validation (or model saving), use this to\n",
        "        restore the former parameters.\n",
        "\n",
        "        Args:\n",
        "          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
        "            updated with the stored parameters.\n",
        "        \"\"\"\n",
        "        for c_param, param in zip(self.collected_params, parameters):\n",
        "            param.data.copy_(c_param.data)\n",
        "\n"
      ],
      "id": "1652ccee"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3894b008"
      },
      "source": [
        "### 5.2. Функции активации"
      ],
      "id": "3894b008"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь определим функцию активации для нашей архитектуры диффузионной модели."
      ],
      "metadata": {
        "id": "7HHHzbNnuNVO"
      },
      "id": "7HHHzbNnuNVO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "182abcbf"
      },
      "outputs": [],
      "source": [
        "def get_act(config=None, act_str=None):\n",
        "    \"\"\"Get activation functions from the config file.\"\"\"\n",
        "    assert (config or act_str) is not None\n",
        "    if config is not None:\n",
        "        act_str = config.model.nonlinearity.lower()\n",
        "    if act_str == 'elu':\n",
        "        return nn.ELU()\n",
        "    elif act_str == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif act_str == 'lrelu':\n",
        "        return nn.LeakyReLU(negative_slope=0.2)\n",
        "    elif act_str == 'swish':\n",
        "        return nn.SiLU()\n",
        "    else:\n",
        "        raise NotImplementedError('activation function does not exist!')"
      ],
      "id": "182abcbf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d3b7826"
      },
      "source": [
        "### 5.3. Эмбеддинги условия (времени)"
      ],
      "id": "9d3b7826"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fef17ac3"
      },
      "source": [
        "Можно включить информацию о времени с помощью гауссовских случайных функций (https://arxiv.org/abs/2006.10739). В частности, мы сначала пробуем $\\omega \\sim \\mathcal{N}(\\mathbf{0}, s ^ 2\\mathbf {I})$, который затем фиксируется для модели (не поддается обучению). Для временного шага $t$ соответствующая гауссова случайная функция определяется как\n",
        "\\begin{align}\n",
        " [\\sin(2\\pi \\omega t) ; \\cos(2\\pi \\omega t)],\n",
        "\\end{align}\n",
        "где $[\\vec{a} ; \\vec{b}]$ обозначает объединение векторов $\\vec{a}$ и $\\vec{b}$. Эта гауссова случайная функция может быть использована в качестве кодировки для временного шага $t$, чтобы сеть score могла определять значение $t$, используя эту кодировку. Мы увидим это далее в коде."
      ],
      "id": "fef17ac3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ad52b3"
      },
      "outputs": [],
      "source": [
        "def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
        "    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(max_positions) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = F.pad(emb, (0, 1), mode='constant')\n",
        "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
        "    return emb"
      ],
      "id": "e2ad52b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5651ec18"
      },
      "source": [
        "### 5.4. Инициализация параметров моделей"
      ],
      "id": "5651ec18"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зададим функцию, которая определяет начальную инициализацию весов нейросетевой модели."
      ],
      "metadata": {
        "id": "YiCrvsP5uVaf"
      },
      "id": "YiCrvsP5uVaf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba2f8309"
      },
      "outputs": [],
      "source": [
        "def variance_scaling(scale, mode, distribution,\n",
        "                     in_axis=1, out_axis=0,\n",
        "                     dtype=torch.float32,\n",
        "                     device='cpu'):\n",
        "    \"\"\"Ported from JAX. \"\"\"\n",
        "\n",
        "    def _compute_fans(shape, in_axis=1, out_axis=0):\n",
        "        receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n",
        "        fan_in = shape[in_axis] * receptive_field_size\n",
        "        fan_out = shape[out_axis] * receptive_field_size\n",
        "        return fan_in, fan_out\n",
        "\n",
        "    def init(shape, dtype=dtype, device=device):\n",
        "        fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n",
        "        if mode == \"fan_in\":\n",
        "            denominator = fan_in\n",
        "        elif mode == \"fan_out\":\n",
        "            denominator = fan_out\n",
        "        elif mode == \"fan_avg\":\n",
        "            denominator = (fan_in + fan_out) / 2\n",
        "        else:\n",
        "            raise ValueError(\n",
        "        \"invalid mode for variance scaling initializer: {}\".format(mode))\n",
        "        variance = scale / denominator\n",
        "        if distribution == \"normal\":\n",
        "            return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n",
        "        elif distribution == \"uniform\":\n",
        "            return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n",
        "        else:\n",
        "            raise ValueError(\"invalid distribution for variance scaling initializer\")\n",
        "\n",
        "    return init"
      ],
      "id": "ba2f8309"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca7736be"
      },
      "outputs": [],
      "source": [
        "def default_init(scale=1.):\n",
        "    \"\"\"The same initialization used in DDPM.\"\"\"\n",
        "    scale = 1e-10 if scale == 0 else scale\n",
        "    return variance_scaling(scale, 'fan_avg' , 'uniform' )"
      ],
      "id": "ca7736be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aa196c9"
      },
      "source": [
        "### 5.5. Сверточная нейронная сеть"
      ],
      "id": "4aa196c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ccdc5a"
      },
      "source": [
        "Следующая функция возвращает сверточную нейронную сеть:\n",
        "\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "- https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html\n",
        "\n",
        "\n",
        "- размер ядер 3 на 3\n",
        "- размер пэддингов (вставки по углам) 1\n",
        "- размер страйдов (величина сдвига ядра) 1\n",
        "- дилэйшен"
      ],
      "id": "f1ccdc5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02e4c6f8"
      },
      "outputs": [],
      "source": [
        "def ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n",
        "    \"\"\"3x3 convolution with DDPM initialization.\"\"\"\n",
        "    conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n",
        "                   dilation=dilation, bias=bias)\n",
        "    conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n",
        "    nn.init.zeros_(conv.bias)\n",
        "    return conv"
      ],
      "id": "02e4c6f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76315ec8"
      },
      "source": [
        "### 5.6. Механизм внимания"
      ],
      "id": "76315ec8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55e59847"
      },
      "source": [
        "Сверточный модуль внимания (англ. сonvolutional block attention module) — простой, но эффективный модуль внимания для сверточных нейросетей. Применяется для задач детектирования обьектов на изображениях и классификации с входными данными больших размерностей. Данный модуль внимания состоит из двух последовательно применяемых подмодулей — канального (применяется ко всем каналам одного пикселя с изображения) и пространственного (применяется ко всему изображению с фиксированным каналом).\n",
        "\n",
        "Напоминаем о том, что есть механизм внимания https://habr.com/ru/articles/458992/"
      ],
      "id": "55e59847"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20bafcae"
      },
      "outputs": [],
      "source": [
        "def _einsum(a, b, c, x, y):\n",
        "    einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n",
        "    return torch.einsum(einsum_str, x, y)\n",
        "\n",
        "\n",
        "def contract_inner(x, y):\n",
        "    \"\"\"tensordot(x, y, 1).\"\"\"\n",
        "    x_chars = list(string.ascii_lowercase[:len(x.shape)])\n",
        "    y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n",
        "    y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n",
        "    out_chars = x_chars[:-1] + y_chars[1:]\n",
        "    return _einsum(x_chars, y_chars, out_chars, x, y)"
      ],
      "id": "20bafcae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39e18b43"
      },
      "outputs": [],
      "source": [
        "class NIN(nn.Module):\n",
        "    def __init__(self, in_dim, num_units, init_scale=0.1):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(default_init(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n",
        "        self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        y = contract_inner(x, self.W) + self.b\n",
        "        return y.permute(0, 3, 1, 2)"
      ],
      "id": "39e18b43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4131ebe"
      },
      "outputs": [],
      "source": [
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"Channel-wise self-attention block.\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n",
        "        self.NIN_0 = NIN(channels, channels)\n",
        "        self.NIN_1 = NIN(channels, channels)\n",
        "        self.NIN_2 = NIN(channels, channels)\n",
        "        self.NIN_3 = NIN(channels, channels, init_scale=0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.GroupNorm_0(x)\n",
        "        q = self.NIN_0(h)\n",
        "        k = self.NIN_1(h)\n",
        "        v = self.NIN_2(h)\n",
        "\n",
        "        w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n",
        "        w = torch.reshape(w, (B, H, W, H * W))\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = torch.reshape(w, (B, H, W, H, W))\n",
        "        h = torch.einsum('bhwij,bcij->bchw', w, v)\n",
        "        h = self.NIN_3(h)\n",
        "        return x + h"
      ],
      "id": "b4131ebe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fc4464"
      },
      "source": [
        "### 5.7. Downsampling"
      ],
      "id": "51fc4464"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95f1026e"
      },
      "source": [
        "Разберем процедуру $\\textbf{Downsampling}$.\n",
        "\n",
        "Во-первых,  стоит понимать, что процедура Downsapmling может быть как нейросетевой, так и детерминистической.\n",
        "\n",
        "1. Детерминистическая процедура представляет собой применение  $\\textbf{Average pooling}$ с размером ядра 2 и страйдом 2. Так мы усредняем значения в каждом квадрате 2 на 2, а значит, в 2 раза уменьшаем размер изображения.\n",
        "\n",
        "2. Нейросетевая процедура. Здесь мы вместо $\\textbf{Average pooling}$ заводим сверточную нейронную сеть с ядром  3 и страйдом 2. Таким образом, на выходе тоже получаем объект в 2 раза меньше исходного."
      ],
      "id": "95f1026e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66e10b6d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, channels, with_conv=False):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.Conv_0 = ddpm_conv3x3(channels, channels, stride=2, padding=0)\n",
        "        self.with_conv = with_conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # Emulate 'SAME' padding\n",
        "        if self.with_conv:\n",
        "            x = F.pad(x, (0, 1, 0, 1))\n",
        "            x = self.Conv_0(x)\n",
        "        else:\n",
        "            x = F.avg_pool2d(x, kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        assert x.shape == (B, C, H // 2, W // 2)\n",
        "        return x"
      ],
      "id": "66e10b6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34be0caf"
      },
      "source": [
        "### 5.8. Upsampling"
      ],
      "id": "34be0caf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d45de4"
      },
      "source": [
        "Класс Upsampling позволяет определить нейронно-сверточную процедуру:\n",
        "    \n",
        "- Если процедура происходит за счет несверточного метода, то размер изображения интерполируется в 2 раза.\n",
        "\n",
        "- Если процедура сверточная: после того как мы растянули изображение в 2 раза, применяем базовую свертку 3 на 3.\n",
        "\n",
        "**Вопрос**: что делает базовая свертка 3 на 3?\n"
      ],
      "id": "46d45de4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cfa0c29"
      },
      "outputs": [],
      "source": [
        "class Upsample(nn.Module):\n",
        "    def __init__(self, channels, with_conv=False):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.Conv_0 = ddpm_conv3x3(channels, channels)\n",
        "        self.with_conv = with_conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = F.interpolate(x, (H * 2, W * 2), mode='nearest')\n",
        "        if self.with_conv:\n",
        "            h = self.Conv_0(h)\n",
        "        return h\n"
      ],
      "id": "5cfa0c29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb60ef0e"
      },
      "source": [
        "### 5.9. ResNet Block"
      ],
      "id": "fb60ef0e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь определим архитектуру сети, лежащей в основе диффузионной модели — ResNet, с встроенной в нее Group norm."
      ],
      "metadata": {
        "id": "AfUPi7Joxnzq"
      },
      "id": "AfUPi7Joxnzq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f937574"
      },
      "source": [
        "![title](https://www.baeldung.com/wp-content/uploads/sites/4/2024/02/group-normalization-and-other-approaches-1024x272.png)"
      ],
      "id": "7f937574"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d91cffe5"
      },
      "outputs": [],
      "source": [
        "class ResnetBlockDDPM(nn.Module):\n",
        "    \"\"\"The ResNet Blocks used in DDPM.\"\"\"\n",
        "    def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if out_ch is None:\n",
        "            out_ch = in_ch\n",
        "        self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=in_ch, eps=1e-6)\n",
        "        self.act = act\n",
        "        self.Conv_0 = ddpm_conv3x3(in_ch, out_ch)\n",
        "        if temb_dim is not None:\n",
        "            self.Dense_0 = nn.Linear(temb_dim, out_ch)\n",
        "            self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n",
        "            nn.init.zeros_(self.Dense_0.bias)\n",
        "\n",
        "        self.GroupNorm_1 = nn.GroupNorm(num_groups=32, num_channels=out_ch, eps=1e-6)\n",
        "        self.Dropout_0 = nn.Dropout(dropout)\n",
        "        self.Conv_1 = ddpm_conv3x3(out_ch, out_ch, init_scale=0.)\n",
        "        if in_ch != out_ch:\n",
        "            if conv_shortcut:\n",
        "                self.Conv_2 = ddpm_conv3x3(in_ch, out_ch)\n",
        "            else:\n",
        "                self.NIN_0 = NIN(in_ch, out_ch)\n",
        "        self.out_ch = out_ch\n",
        "        self.in_ch = in_ch\n",
        "        self.conv_shortcut = conv_shortcut\n",
        "\n",
        "    \"\"\"\n",
        "    def forward(self, x, temb=None):\n",
        "        return checkpoint(self._forward, (x,) ,self.parameters(), use_checkpoint)\n",
        "    \"\"\"\n",
        "    def forward(self, x, temb=None):\n",
        "        B, C, H, W = x.shape\n",
        "        assert C == self.in_ch\n",
        "        out_ch = self.out_ch if self.out_ch else self.in_ch\n",
        "        h = self.act(self.GroupNorm_0(x))\n",
        "        h = self.Conv_0(h)\n",
        "        # Add bias to each feature map conditioned on the time embedding\n",
        "        if temb is not None:\n",
        "            h += self.Dense_0(self.act(temb))[:, :, None, None]\n",
        "        h = self.act(self.GroupNorm_1(h))\n",
        "        h = self.Dropout_0(h)\n",
        "        h = self.Conv_1(h)\n",
        "        if C != out_ch:\n",
        "            if self.conv_shortcut:\n",
        "                x = self.Conv_2(x)\n",
        "            else:\n",
        "                x = self.NIN_0(x)\n",
        "        return x + h"
      ],
      "id": "d91cffe5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe9aa92e"
      },
      "source": [
        "### 5.10 Model DDPM"
      ],
      "id": "fe9aa92e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce15eb94"
      },
      "outputs": [],
      "source": [
        "class DDPM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.act = act = get_act(config)\n",
        "\n",
        "        self.nf = nf = config.model.nf # dimensionality of time embedding\n",
        "        self.conditional = conditional = config.model.conditional  # time conditional for diffusion  process\n",
        "\n",
        "        modules = [] # This list is composed of nets\n",
        "        if conditional:\n",
        "            # Condition on noise levels.\n",
        "            modules = [nn.Linear(nf, nf * 4)]\n",
        "            modules[0].weight.data = default_init()(modules[0].weight.data.shape)\n",
        "            nn.init.zeros_(modules[0].bias)\n",
        "            modules.append(nn.Linear(nf * 4, nf * 4))\n",
        "            modules[1].weight.data = default_init()(modules[1].weight.data.shape)\n",
        "            nn.init.zeros_(modules[1].bias)\n",
        "\n",
        "        self.centered = config.data.centered\n",
        "        channels = config.data.num_channels\n",
        "\n",
        "\n",
        "        # downsampling block #\n",
        "        modules.append(ddpm_conv3x3(channels, nf))\n",
        "        ch_mult = config.model.ch_mult\n",
        "        dropout = config.model.dropout\n",
        "        self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n",
        "        self.num_resolutions = num_resolutions = len(ch_mult) # downsamples\n",
        "        # all_resolutions: [16,8,4,2]\n",
        "        self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n",
        "        self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n",
        "        resamp_with_conv = config.model.resamp_with_conv\n",
        "\n",
        "\n",
        "        ResnetBlock = functools.partial(ResnetBlockDDPM, act=act, temb_dim=4 * nf, dropout=dropout)\n",
        "        from   ContDDPM.models.ddpm_entities import AttnBlock\n",
        "        AttnBlock = functools.partial(AttnBlock)\n",
        "\n",
        "        #####################\n",
        "        # Downsampling block#\n",
        "        #####################\n",
        "        hs_c = [nf]\n",
        "        in_ch = nf\n",
        "\n",
        "        for i_level in range(num_resolutions):\n",
        "\n",
        "            # Residual blocks for this resolution\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                out_ch = nf * ch_mult[i_level]\n",
        "                modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n",
        "                in_ch = out_ch\n",
        "                if all_resolutions[i_level] in attn_resolutions:\n",
        "                    modules.append(AttnBlock(channels=in_ch))\n",
        "                hs_c.append(in_ch)\n",
        "\n",
        "            if i_level != num_resolutions - 1:\n",
        "                modules.append(Downsample(channels=in_ch, with_conv=resamp_with_conv))\n",
        "                hs_c.append(in_ch)\n",
        "\n",
        "        in_ch = hs_c[-1]\n",
        "        modules.append(ResnetBlock(in_ch=in_ch))\n",
        "        modules.append(AttnBlock(channels=in_ch))\n",
        "        modules.append(ResnetBlock(in_ch=in_ch))\n",
        "        #####################\n",
        "\n",
        "\n",
        "        #####################\n",
        "        # Upsampling block#\n",
        "        #####################\n",
        "        for i_level in reversed(range(num_resolutions)):\n",
        "            for i_block in range(num_res_blocks + 1):\n",
        "                out_ch = nf * ch_mult[i_level]\n",
        "                modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(), out_ch=out_ch))\n",
        "                in_ch = out_ch\n",
        "            if all_resolutions[i_level] in attn_resolutions:\n",
        "                modules.append(AttnBlock(channels=in_ch))\n",
        "            if i_level != 0:\n",
        "                modules.append(Upsample(channels=in_ch, with_conv=resamp_with_conv))\n",
        "        #####################\n",
        "\n",
        "\n",
        "        assert not hs_c\n",
        "        modules.append(nn.GroupNorm(num_channels=in_ch, num_groups=32, eps=1e-6))\n",
        "        modules.append(ddpm_conv3x3(in_ch, channels, init_scale=0.))\n",
        "        self.all_modules = nn.ModuleList(modules)\n",
        "\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "\n",
        "        \"\"\"\n",
        "        Running Diffusion model\n",
        "\n",
        "\n",
        "        inputs:\n",
        "        - x: [B,C,H,W] = [B,3,16,16] images\n",
        "        - labels: [B]                time\n",
        "\n",
        "        returns:\n",
        "        \"\"\"\n",
        "\n",
        "        modules = self.all_modules\n",
        "        m_idx = 0\n",
        "\n",
        "\n",
        "        ##################\n",
        "        #Time embeddings #\n",
        "        ##################\n",
        "        if self.conditional:\n",
        "            # timestep/scale embedding\n",
        "            timesteps = labels # torch.Size([B])\n",
        "            temb = get_timestep_embedding(timesteps, self.nf) # torch.Size([B, nf])\n",
        "            # modules[0] = torch.nn.Linear(nf, nf*4)\n",
        "            temb = modules[m_idx](temb) # torch.Size([B, nf*4])\n",
        "            m_idx += 1\n",
        "            # modules[1] = torch.nn.Linear(nf*4,nf*4) = torch.nn.Linear(512,512)\n",
        "            temb = modules[m_idx](self.act(temb)) # torch.Size([B, nf*4])\n",
        "            m_idx += 1\n",
        "        else:\n",
        "            temb = None\n",
        "        #################\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "        #Centering of Images #\n",
        "        ######################\n",
        "        if self.centered:\n",
        "            # Input is in [-1, 1]\n",
        "            h = x\n",
        "\n",
        "            #assert torch.min(x).item() >= -1.1\n",
        "            #assert torch.max(x).item() <=  1.1\n",
        "        else:\n",
        "            # Input is in [0, 1]\n",
        "            h = 2 * x - 1.\n",
        "            #assert torch.min(x).item() >= 0.1\n",
        "            #assert torch.max(x).item() <= 1.1\n",
        "        #################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #####################\n",
        "        # Downsampling block#\n",
        "        #####################\n",
        "\n",
        "        # torch.nn.Conv2D(3, nf, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        hs = [modules[m_idx](h)] # torch.Size([B,nf,16,16])\n",
        "        m_idx += 1\n",
        "\n",
        "        for i_level in range(self.num_resolutions): #self.num_resolutions\n",
        "\n",
        "            # Residual blocks for this resolution\n",
        "\n",
        "            #################################\n",
        "            ##### while resolution = 16 #####\n",
        "            ##################################\n",
        "\n",
        "\n",
        "            # 1. ResNetBlock(in_ch=nf,out_ch = nf*1 = nf*ch_mult[0]) -> [torch.Size([B,nf,16,16])]\n",
        "            # 2. AttnBlock -> [torch.Size([B,nf,16,16])]\n",
        "            # 3. ResNetBlock(in_ch=nf,out_ch = nf*1 = nf*ch_mult[0]) -> [torch.Size([B,nf,16,16])]\n",
        "            # 4. AttnBlock -> [torch.Size([B,nf,16,16])]\n",
        "            # 5. ResNetBlock(in_ch=nf,out_ch = nf*1 = nf*ch_mult[0]) -> [torch.Size([B,nf,16,16])]\n",
        "            # 6. AttnBlock -> [torch.Size([B,nf,16,16])]\n",
        "            # 7. ResNetBlock(in_ch=nf,out_ch = nf*1 = nf*ch_mult[0]) -> [torch.Size([B,nf,16,16])]\n",
        "            # 8. AttnBlock -> [torch.Size([B,nf,16,16])]\n",
        "            # 9. DownSampling -> [torch.Size([B,nf,8,8])]\n",
        "            # len(hs) = 6 : hs = [before, after_attn_1, ...., after_attn_4, after_downsampling]\n",
        "\n",
        "            ##################################\n",
        "\n",
        "\n",
        "            #################################\n",
        "            ##### while resolution = 8 #####\n",
        "            ##################################\n",
        "\n",
        "\n",
        "\n",
        "            #################################\n",
        "\n",
        "\n",
        "            #4 times ResNet(in_ch=nf,out_ch=nf), 4 times hs.append(torch.Size([B,nf,16,16]))\n",
        "            # Downsample step: hs.append([torch.Size([B,nf,8,8])])\n",
        "\n",
        "            # while resolution = 8: ResNet(in_ch=nf,out_ch=nf*2) and Attn(nf*2) , 4 times hs.append(torch.Size([B,nf*2,8,8]))\n",
        "            # Downsample step: hs.append([torch.Size([B,nf*2,4,4])])\n",
        "\n",
        "            # while resolution = 4: 4 times ResNet(in_ch=nf*2,out_ch=nf*2), 4 times hs.append(torch.Size([B,nf*2,4,4]))\n",
        "            # Downsample step: hs.append([torch.Size([B,nf*2,2,2])])\n",
        "\n",
        "            # while resolution = 2: 4 times ResNet(in_ch=nf*2,out_ch=nf*2), 4 times hs.append(torch.Size([B,nf*2,2,2]))\n",
        "            # without last Downsample step\n",
        "\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = modules[m_idx](hs[-1], temb)\n",
        "\n",
        "                m_idx += 1\n",
        "                if h.shape[-1] in self.attn_resolutions:\n",
        "                    # Application of Attention block\n",
        "                    h = modules[m_idx](h)\n",
        "                    m_idx += 1\n",
        "                hs.append(h)\n",
        "\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                # Application of DownSample block\n",
        "                hs.append(modules[m_idx](hs[-1]))\n",
        "                m_idx += 1\n",
        "\n",
        "        # hs[-1] = torch.Size([B,nf*2,2,2])\n",
        "        # temb  = torch.Size([B,nf*4])\n",
        "\n",
        "\n",
        "        h = hs[-1]\n",
        "        h = modules[m_idx](h, temb) # torch.Size([B,nf*2,2,2])\n",
        "        m_idx += 1\n",
        "        h = modules[m_idx](h) # torch.Size([B,nf*2,2,2])\n",
        "        m_idx += 1\n",
        "        h = modules[m_idx](h, temb)# torch.Size([B,nf*2,2,2])\n",
        "        m_idx += 1\n",
        "        #####################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #####################\n",
        "        # Upsampling block#\n",
        "        #####################\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                m_idx += 1\n",
        "\n",
        "            if h.shape[-1] in self.attn_resolutions:\n",
        "                h = modules[m_idx](h)\n",
        "                m_idx += 1\n",
        "            if i_level != 0:\n",
        "                h = modules[m_idx](h)\n",
        "                m_idx += 1\n",
        "\n",
        "        assert not hs\n",
        "        h = self.act(modules[m_idx](h))\n",
        "        m_idx += 1\n",
        "        h = modules[m_idx](h)\n",
        "        m_idx += 1\n",
        "        assert m_idx == len(modules)\n",
        "\n",
        "        return h\n"
      ],
      "id": "ce15eb94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64c12125"
      },
      "outputs": [],
      "source": [
        "config = # создайте ваш конфиг\n",
        "config.data.image_size=16\n",
        "model = DDPM(config )"
      ],
      "id": "64c12125"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9006cd0e",
        "outputId": "bb3abda2-9d5b-4a6d-f214-8199cc5894ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Module.modules of DDPM(\n",
              "  (act): SiLU()\n",
              "  (all_modules): ModuleList(\n",
              "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (2): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (4): AttnBlock(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (NIN_0): NIN()\n",
              "      (NIN_1): NIN()\n",
              "      (NIN_2): NIN()\n",
              "      (NIN_3): NIN()\n",
              "    )\n",
              "    (5): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (6): AttnBlock(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (NIN_0): NIN()\n",
              "      (NIN_1): NIN()\n",
              "      (NIN_2): NIN()\n",
              "      (NIN_3): NIN()\n",
              "    )\n",
              "    (7): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (8): AttnBlock(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (NIN_0): NIN()\n",
              "      (NIN_1): NIN()\n",
              "      (NIN_2): NIN()\n",
              "      (NIN_3): NIN()\n",
              "    )\n",
              "    (9): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (10): AttnBlock(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (NIN_0): NIN()\n",
              "      (NIN_1): NIN()\n",
              "      (NIN_2): NIN()\n",
              "      (NIN_3): NIN()\n",
              "    )\n",
              "    (11): Downsample(\n",
              "      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "    )\n",
              "    (12): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (13-15): 3 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (16): Downsample(\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "    )\n",
              "    (17-20): 4 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (21): Downsample(\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "    )\n",
              "    (22-26): 5 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (27): AttnBlock(\n",
              "      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (NIN_0): NIN()\n",
              "      (NIN_1): NIN()\n",
              "      (NIN_2): NIN()\n",
              "      (NIN_3): NIN()\n",
              "    )\n",
              "    (28): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (29-33): 5 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (34): Upsample(\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (35-39): 5 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (40): Upsample(\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (41-44): 4 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (45): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 384, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (46): Upsample(\n",
              "      (Conv_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (47): ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 384, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (48-51): 4 x ResnetBlockDDPM(\n",
              "      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "      (act): SiLU()\n",
              "      (Conv_0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (Dense_0): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (Dropout_0): Dropout(p=0.1, inplace=False)\n",
              "      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (NIN_0): NIN()\n",
              "    )\n",
              "    (52): AttnBlock(\n",
              "      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "      (NIN_0): NIN()\n",
              "      (NIN_1): NIN()\n",
              "      (NIN_2): NIN()\n",
              "      (NIN_3): NIN()\n",
              "    )\n",
              "    (53): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "    (54): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")>"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.modules"
      ],
      "id": "9006cd0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ad85eec"
      },
      "outputs": [],
      "source": [],
      "id": "8ad85eec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e5e3c1d"
      },
      "source": [
        "## 6. Данные"
      ],
      "id": "8e5e3c1d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данная функция берет картинки из датасета MNIST. Раскрасим цифры разными цветами, созранив темный бэкграунд:\n",
        "\n",
        "![title](https://iimg.su/s/18/XP2QCS85msDHHaIf7TFJvfxf3wt7htnWNqF4EyJR.png)"
      ],
      "metadata": {
        "id": "4Z8H1gtfRRSu"
      },
      "id": "4Z8H1gtfRRSu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "246c014c"
      },
      "outputs": [],
      "source": [
        "def get_random_colored_images(images, seed = 0x000000):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    images = 0.5*(images + 1)\n",
        "    size = images.shape[0]\n",
        "    colored_images = []\n",
        "    hues = 360*np.random.rand(size)\n",
        "\n",
        "    for V, H in zip(images, hues):\n",
        "        V_min = 0\n",
        "\n",
        "        a = (V - V_min)*(H%60)/60\n",
        "        V_inc = a\n",
        "        V_dec = V - a\n",
        "\n",
        "        colored_image = torch.zeros((3, V.shape[1], V.shape[2]))\n",
        "        H_i = round(H/60) % 6\n",
        "\n",
        "        if H_i == 0:\n",
        "            colored_image[0] = V\n",
        "            colored_image[1] = V_inc\n",
        "            colored_image[2] = V_min\n",
        "        elif H_i == 1:\n",
        "            colored_image[0] = V_dec\n",
        "            colored_image[1] = V\n",
        "            colored_image[2] = V_min\n",
        "        elif H_i == 2:\n",
        "            colored_image[0] = V_min\n",
        "            colored_image[1] = V\n",
        "            colored_image[2] = V_inc\n",
        "        elif H_i == 3:\n",
        "            colored_image[0] = V_min\n",
        "            colored_image[1] = V_dec\n",
        "            colored_image[2] = V\n",
        "        elif H_i == 4:\n",
        "            colored_image[0] = V_inc\n",
        "            colored_image[1] = V_min\n",
        "            colored_image[2] = V\n",
        "        elif H_i == 5:\n",
        "            colored_image[0] = V\n",
        "            colored_image[1] = V_min\n",
        "            colored_image[2] = V_dec\n",
        "\n",
        "        colored_images.append(colored_image)\n",
        "\n",
        "    colored_images = torch.stack(colored_images, dim = 0)\n",
        "    colored_images = 2*colored_images - 1\n",
        "\n",
        "    return colored_images"
      ],
      "id": "246c014c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующая функция создает генератор для наших данных, батчи из которого мы будем использовать во время обучения."
      ],
      "metadata": {
        "id": "e-Wn5fsdR4Cf"
      },
      "id": "e-Wn5fsdR4Cf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26e5d088"
      },
      "outputs": [],
      "source": [
        "class DataGenerator:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        if self.config.data.dataset.startswith('shoes'):\n",
        "            test_ratio=0.1\n",
        "            dataset = h5py_to_dataset(config.data.path, config.data.image_size)\n",
        "            idx = list(range(len(dataset)))\n",
        "            test_size = int(len(idx) * test_ratio)\n",
        "            train_idx, test_idx = idx[:-test_size], idx[-test_size:]\n",
        "            train_set, test_set = Subset(dataset, train_idx), Subset(dataset, test_idx)\n",
        "\n",
        "            self.train_loader = DataLoader(train_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "            self.valid_loader = DataLoader(test_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "\n",
        "        elif self.config.data.dataset.startswith('celeba_male'):\n",
        "\n",
        "            test_ratio=0.1\n",
        "            transform = Compose([ CenterCrop(140),\n",
        "                                  Resize((config.data.image_size, config.data.image_size)),\n",
        "                                  ToTensor(),\n",
        "                                  #Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                  #Lambda(lambda x: (x+1)/2 )]\n",
        "                                ])\n",
        "\n",
        "            dataset = ImageFolder(config.data.path, transform=transform)\n",
        "            idx = list(range(len(dataset)))\n",
        "            test_size = int(len(idx) * test_ratio)\n",
        "            train_idx, test_idx = idx[:-test_size], idx[-test_size:]\n",
        "            train_set, test_set = Subset(dataset, train_idx), Subset(dataset, test_idx)\n",
        "\n",
        "            self.train_loader = DataLoader(train_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "            self.valid_loader = DataLoader(test_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "\n",
        "        elif self.config.data.dataset.startswith('anime'):\n",
        "            test_ratio=0.1\n",
        "            transform = Compose([Resize((config.data.image_size, config.data.image_size)),\n",
        "                                 ToTensor(),\n",
        "                                 #Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                 ])\n",
        "            dataset = ImageFolder(config.data.path, transform=transform)\n",
        "            idx = list(range(len(dataset)))\n",
        "            test_size = int(len(idx) * test_ratio)\n",
        "            train_idx, test_idx = idx[:-test_size], idx[-test_size:]\n",
        "            train_set, test_set = Subset(dataset, train_idx), Subset(dataset, test_idx)\n",
        "\n",
        "            self.train_loader = DataLoader(train_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "            self.valid_loader = DataLoader(test_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "\n",
        "        elif self.config.data.dataset.startswith('church'):\n",
        "            test_ratio=0.3\n",
        "            dataset = np.load(self.config.data.path)\n",
        "\n",
        "\n",
        "\n",
        "            idx = list(range(len(dataset)))\n",
        "            test_size = int(len(idx) * test_ratio)\n",
        "            train_idx, test_idx = idx[:-test_size], idx[-test_size:]\n",
        "\n",
        "            train_set = 2 * (torch.tensor(np.array(dataset[train_idx]), dtype=torch.float32) / 255.).permute(0, 3, 1, 2) - 1\n",
        "            train_set = F.interpolate(train_set,config.data.image_size , mode='bilinear')\n",
        "\n",
        "            test_set = 2 * (torch.tensor(np.array(dataset[test_idx]), dtype=torch.float32) / 255.).permute(0, 3, 1, 2) - 1\n",
        "            test_set = F.interpolate(test_set,config.data.image_size , mode='bilinear')\n",
        "\n",
        "            train_set, test_set = Subset(train_set, train_idx), Subset(test_set, test_idx)\n",
        "\n",
        "            self.train_loader = DataLoader(train_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "            self.valid_loader = DataLoader(test_set, shuffle=True, num_workers=0, batch_size=config.data.batch_size)\n",
        "\n",
        "        else:\n",
        "            self.train_mnist_transforms = Compose(\n",
        "                [\n",
        "                    # Resize((config.data.image_size, config.data.image_size)),\n",
        "                    Resize((config.data.image_size, config.data.image_size)),\n",
        "                    ToTensor(),\n",
        "                    #Normalize(mean=config.data.norm_mean, std=config.data.norm_std),\n",
        "                    # to [-1; 1]\n",
        "                    Lambda(lambda x:  2*x-1 )\n",
        "                ]\n",
        "            )\n",
        "\n",
        "\n",
        "            self.valid_mnist_transforms = Compose(\n",
        "                [\n",
        "                    # Resize((config.data.image_size, config.data.image_size)),\n",
        "                    Resize((config.data.image_size, config.data.image_size)),\n",
        "                    ToTensor(),\n",
        "                    #Normalize(mean=config.data.norm_mean, std=config.data.norm_std),\n",
        "                    # to [-1; 1]\n",
        "                    Lambda(lambda x: 2*x-1 )\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            dataset_name = config.data.dataset.split(\"_\")[0]\n",
        "            is_colored = dataset_name[-7:] == \"colored\"\n",
        "            classes = [int(number) for number in config.data.dataset.split(\"_\")[1:]]\n",
        "            if not classes:\n",
        "                classes = [i for i in range(10)]\n",
        "\n",
        "            train_set =  MNIST(config.data.path, train=True, transform=self.train_mnist_transforms, download=True)\n",
        "            test_set =  MNIST(config.data.path, train=False, transform=self.valid_mnist_transforms, download=True)\n",
        "\n",
        "            train_test = []\n",
        "            for dataset in [train_set, test_set]:\n",
        "                data = []\n",
        "                labels = []\n",
        "                for k in range(len(classes)):\n",
        "                    data.append(torch.stack(\n",
        "                        [dataset[i][0] for i in range(len(dataset.targets)) if dataset.targets[i] == classes[k]],\n",
        "                        dim=0\n",
        "                    ))\n",
        "                    labels += [k]*data[-1].shape[0]\n",
        "                data = torch.cat(data, dim=0)\n",
        "                data = data.reshape(-1, 1, config.data.image_size, config.data.image_size)\n",
        "                labels = torch.tensor(labels)\n",
        "\n",
        "                if is_colored:\n",
        "                    data = get_random_colored_images(data)\n",
        "\n",
        "                train_test.append(TensorDataset(data, labels))\n",
        "\n",
        "            train_set, test_set = train_test\n",
        "\n",
        "\n",
        "\n",
        "            self.train_loader = DataLoader(\n",
        "                train_set,\n",
        "                batch_size=config.training.batch_size,\n",
        "                shuffle=True,\n",
        "                drop_last=True\n",
        "            )\n",
        "\n",
        "\n",
        "            self.valid_loader = DataLoader(\n",
        "                test_set,\n",
        "                batch_size= config.training.batch_size,\n",
        "                shuffle=False,\n",
        "                drop_last=False\n",
        "            )\n",
        "\n",
        "    def sample_train(self):\n",
        "        while True:\n",
        "            for batch in self.train_loader:\n",
        "                yield batch\n"
      ],
      "id": "26e5d088"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0472491"
      },
      "source": [
        "## 7. Diffusion Runner"
      ],
      "id": "f0472491"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](https://habrastorage.org/getpro/habr/upload_files/de1/754/3a4/de17543a4f693c78f50f6f93b231374b.png)\n",
        "\n",
        "\n",
        "Пайплайн модели:\n",
        "\n",
        "- Семплируем батч данных\n",
        "- Выбираем различные уровни зашумления\n",
        "- Зашумляем наши данные из батча\n",
        "- По зашумленным точкам вычисляем score-функцию $\\nabla \\log p(x)$\n",
        "- Запускаем обратный процесс семплирования\n"
      ],
      "metadata": {
        "id": "fPkRsJAcx6Jh"
      },
      "id": "fPkRsJAcx6Jh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd0d5cc9"
      },
      "outputs": [],
      "source": [
        "class DiffusionRunner:\n",
        "    def __init__(\n",
        "            self,\n",
        "            config: ConfigDict,\n",
        "            eval: bool = False\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.eval = eval\n",
        "\n",
        "        self.model = DDPM(config=config)\n",
        "        self.sde = VP_SDE(config=config)\n",
        "        self.rsde = RSDE(self.sde, ode_sampling=config.training.ode_sampling)\n",
        "        self.diff_eq_solver = EulerDiffEqSolver(self.sde, self.rsde,\n",
        "                                                self.calc_score,\n",
        "                                                ode_sampling=config.training.ode_sampling)\n",
        "\n",
        "        #self.inverse_scaler = lambda x: torch.clip(127.5 * (x + 1), 0, 255)\n",
        "        self.inverse_scaler = lambda x: torch.clip( 255 * x, 0, 255)\n",
        "\n",
        "        self.checkpoints_folder = config.training.checkpoints_folder\n",
        "        if eval:\n",
        "            self.ema = ExponentialMovingAverage(self.model.parameters(), config.model.ema_rate)\n",
        "            self.restore_parameters()\n",
        "            self.switch_to_ema()\n",
        "\n",
        "        device = torch.device(self.config.device)\n",
        "        self.device = device\n",
        "        self.model.to(device)\n",
        "\n",
        "    def restore_parameters(self, device: Optional[torch.device] = None) -> None:\n",
        "        checkpoints_folder: str = self.checkpoints_folder\n",
        "        if device is None:\n",
        "            device = torch.device('cpu')\n",
        "        model_ckpt = torch.load(checkpoints_folder + '/model.pth', map_location=device)\n",
        "        self.model.load_state_dict(model_ckpt)\n",
        "\n",
        "        ema_ckpt = torch.load(checkpoints_folder + '/ema.pth', map_location=device)\n",
        "        self.ema.load_state_dict(ema_ckpt)\n",
        "\n",
        "    def switch_to_ema(self) -> None:\n",
        "        ema = self.ema\n",
        "        score_model = self.model\n",
        "        ema.store(score_model.parameters())\n",
        "        ema.copy_to(score_model.parameters())\n",
        "\n",
        "    def switch_back_from_ema(self) -> None:\n",
        "        ema = self.ema\n",
        "        score_model = self.model\n",
        "        ema.restore(score_model.parameters())\n",
        "\n",
        "    def set_optimizer(self) -> None:\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=self.config.optim.lr,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=self.config.optim.weight_decay\n",
        "        )\n",
        "        self.warmup = self.config.optim.linear_warmup\n",
        "        self.grad_clip_norm = self.config.optim.grad_clip_norm\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def calc_score(self, input_x: torch.Tensor, input_t: torch.Tensor, y=None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        calculate score w.r.t noisy X and t\n",
        "        \"\"\"\n",
        "        model_output = self.model(input_x, input_t)\n",
        "        curr_std = self.sde.marginal_std(input_t)\n",
        "\n",
        "        score = - model_output / curr_std[:, None, None, None]\n",
        "        return score\n",
        "\n",
        "    def sample_time(self, batch_size: int, eps: float = 1e-5):\n",
        "        return torch.rand(batch_size) * (self.sde.T - eps) + eps\n",
        "\n",
        "    def calc_loss(self, clean_x: torch.Tensor, eps: float = 1e-5) -> Union[float, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Define score-matching MSE loss\n",
        "        \"\"\"\n",
        "        # здесь сэмплируем время, прогоняем через сетку и рассчитываем MSE-лосс\n",
        "\n",
        "        batch_size = clean_x.shape[0]\n",
        "        t = self.sample_time(batch_size, eps).to(self.device) # sample time\n",
        "\n",
        "        noise = self.sde.prior_sampling(clean_x.shape).to(self.device)  # noise batch from N(0, I)\n",
        "\n",
        "        # forward_pass + зашумление\n",
        "        mean, std = self.sde.marginal_prob(clean_x, t)\n",
        "        noised_x = mean + std[:, None, None, None] * noise\n",
        "\n",
        "        # score_estimation\n",
        "\n",
        "        rebuilt_noise = self.model(noised_x, t)\n",
        "\n",
        "        # сalc_loss\n",
        "        loss = F.mse_loss(rebuilt_noise, noise)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def set_data_generator(self) -> None:\n",
        "        self.datagen = DataGenerator(self.config)\n",
        "\n",
        "    def manage_optimizer(self) -> None:\n",
        "        self.lrs = []\n",
        "        if self.warmup > 0 and self.step < self.warmup:\n",
        "            for g in self.optimizer.param_groups:\n",
        "                self.lrs += [g['lr']]\n",
        "                g['lr'] = g['lr'] * float(self.step + 1) / self.warmup\n",
        "        if self.grad_clip_norm is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                self.model.parameters(),\n",
        "                max_norm=self.grad_clip_norm\n",
        "            )\n",
        "\n",
        "    def restore_optimizer_state(self) -> None:\n",
        "        if self.lrs:\n",
        "            self.lrs = self.lrs[::-1]\n",
        "            for g in self.optimizer.param_groups:\n",
        "                g['lr'] = self.lrs.pop()\n",
        "\n",
        "    def log_metric(self, metric_name: str, loader_name: str, value: Union[float, torch.Tensor, wandb.Image]):\n",
        "        wandb.log({f'{metric_name}/{loader_name}': value}, step=self.step)\n",
        "\n",
        "    def optimizer_step(self, loss: torch.Tensor) -> None:\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        self.manage_optimizer()\n",
        "        self.log_metric('lr', 'train', self.optimizer.param_groups[0]['lr'])\n",
        "        self.optimizer.step()\n",
        "        self.ema.update(self.model.parameters())\n",
        "        self.restore_optimizer_state()\n",
        "\n",
        "    def validate(self) -> None:\n",
        "        prev_mode= self.model.training\n",
        "\n",
        "        self.model.eval()\n",
        "        self.switch_to_ema()\n",
        "\n",
        "        valid_loss = 0\n",
        "        valid_count = 0\n",
        "        with torch.no_grad():\n",
        "            for (X,y) in self.datagen.valid_loader:\n",
        "                X = X.to(self.device)\n",
        "                loss = self.calc_loss(clean_x=X)\n",
        "                valid_loss += loss.item() * X.size(0)\n",
        "                valid_count += X.size(0)\n",
        "\n",
        "        valid_loss = valid_loss / valid_count\n",
        "        self.log_metric('loss', 'valid_loader', valid_loss)\n",
        "\n",
        "        self.switch_back_from_ema()\n",
        "        self.model.train(prev_mode)\n",
        "\n",
        "    def train(self) -> None:\n",
        "        self.set_optimizer()\n",
        "        self.set_data_generator()\n",
        "        train_generator = self.datagen.sample_train()\n",
        "        self.step = 0\n",
        "\n",
        "        wandb.init(project='sde', name='ddpm_cont')\n",
        "\n",
        "        self.ema = ExponentialMovingAverage(self.model.parameters(), self.config.model.ema_rate)\n",
        "        self.model.train()\n",
        "        for iter_idx in trange(1, 1 + self.config.training.training_iters):\n",
        "            self.step = iter_idx\n",
        "\n",
        "            if self.config.data.dataset == \"church\":\n",
        "                X =  next(train_generator)\n",
        "            else:\n",
        "                (X, y) = next(train_generator)\n",
        "\n",
        "            #print(torch.min(X[0]), torch.max(X[0]) )\n",
        "            X = X.to(self.device)\n",
        "            loss = self.calc_loss(clean_x=X)\n",
        "            self.log_metric('loss', 'train', loss.item())\n",
        "            self.optimizer_step(loss)\n",
        "\n",
        "            if iter_idx % self.config.training.snapshot_freq == 0:\n",
        "                self.snapshot()\n",
        "\n",
        "            if iter_idx % self.config.training.eval_freq == 0:\n",
        "                self.validate()\n",
        "\n",
        "            if iter_idx % self.config.training.checkpoint_freq == 0:\n",
        "                self.save_checkpoint()\n",
        "\n",
        "        self.model.eval()\n",
        "        self.save_checkpoint()\n",
        "        self.switch_to_ema()\n",
        "\n",
        "    def save_checkpoint(self) -> None:\n",
        "        if not os.path.exists(self.checkpoints_folder):\n",
        "            os.makedirs(self.checkpoints_folder)\n",
        "        torch.save(self.model.state_dict(), os.path.join(self.checkpoints_folder,\n",
        "                                                               f'model.pth'))\n",
        "        torch.save(self.ema.state_dict(), os.path.join(self.checkpoints_folder,\n",
        "                                                       f'ema.pth'))\n",
        "        torch.save(self.optimizer.state_dict(), os.path.join(self.checkpoints_folder,\n",
        "                                                             f'opt.pth'))\n",
        "\n",
        "    def reset_unconditional_sampling() -> None:\n",
        "        self.diff_eq_solver = EulerDiffEqSolver(\n",
        "            self.sde,\n",
        "            self.calc_score,\n",
        "            self.config.training.ode_sampling\n",
        "        )\n",
        "\n",
        "    def set_conditional_sampling(\n",
        "            self,\n",
        "            classifier_grad_fn: Callable[[\"NoisyImages\", \"T\", \"Labels\"], \"Scores\"],\n",
        "            T: float = 1.0\n",
        "    ) -> None:\n",
        "        def new_score_fn(x, t, y):\n",
        "            \"\"\"\n",
        "            define posterior_score w.r.t T\n",
        "            \"\"\"\n",
        "            return posterior_score_T\n",
        "\n",
        "        self.diff_eq_solver = EulerDiffEqSolver(\n",
        "            self.sde,\n",
        "            new_score_fn,\n",
        "            self.config.training.ode_sampling\n",
        "        )\n",
        "\n",
        "\n",
        "    def set_classifier(self, classifier: torch.nn.Module, T: float = 1.0) -> None:\n",
        "        self.classifier = classifier\n",
        "        def classifier_grad_fn(x, t, y):\n",
        "            \"\"\"\n",
        "            calculate likelihood_score with torch.autograd.grad\n",
        "            \"\"\"\n",
        "            return likelihood_score\n",
        "\n",
        "        self.set_conditional_sampling(classifier_grad_fn, T=T)\n",
        "\n",
        "\n",
        "    def sample_images(\n",
        "            self, batch_size: int,\n",
        "            eps:float = 1e-5,\n",
        "            labels: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        shape = (\n",
        "            batch_size,\n",
        "            self.config.data.num_channels,\n",
        "            self.config.data.image_size,\n",
        "            self.config.data.image_size\n",
        "        )\n",
        "        device = torch.device(self.config.device)\n",
        "        with torch.no_grad():\n",
        "            \"\"\"\n",
        "            Implement cycle for Euler RSDE sampling w.r.t labels\n",
        "            labels = None if uncond. gen is used\n",
        "            \"\"\"\n",
        "\n",
        "            # сэмплируем нормальный шум\n",
        "            pred_images = self.sde.prior_sampling(shape).to(self.device)\n",
        "            # делаем временную сетку размера N\n",
        "            timesteps = torch.linspace(self.sde.T, eps, self.sde.N, device=self.device)\n",
        "            # применяем метод Эйлера\n",
        "            for i in range(self.sde.N):\n",
        "                # делаем батч для времени, устанавливая везде текущее время\n",
        "                time = torch.ones(batch_size, device=self.device) * timesteps[i]\n",
        "                # делаем шаг метода Эйлера\n",
        "                pred_images, _ = self.diff_eq_solver.step(pred_images, time, y=labels)\n",
        "\n",
        "        return self.inverse_scaler(pred_images)\n",
        "\n",
        "\n",
        "    def snapshot(self, labels: Optional[torch.Tensor] = None) -> None:\n",
        "        prev_mode = self.model.training\n",
        "\n",
        "        self.model.eval()\n",
        "        self.switch_to_ema()\n",
        "\n",
        "        images = self.sample_images(self.config.training.snapshot_batch_size, labels=labels).cpu()\n",
        "        nrow = int(math.sqrt(self.config.training.snapshot_batch_size))\n",
        "        grid = torchvision.utils.make_grid(images, nrow=nrow).permute(1, 2, 0)\n",
        "        grid = grid.data.numpy().astype(np.uint8)\n",
        "        self.log_metric('images', 'from_noise', wandb.Image(grid))\n",
        "\n",
        "        self.switch_back_from_ema()\n",
        "        self.model.train(prev_mode)\n",
        "\n",
        "    def train_classifier(\n",
        "            self,\n",
        "            classifier: torch.nn.Module,\n",
        "            classifier_optim: torch.optim.Optimizer,\n",
        "            classifier_loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "            T: float = 10.0\n",
        "    ) -> None:\n",
        "        device = torch.device(self.config.device)\n",
        "        self.device = device\n",
        "\n",
        "        self.set_classifier(classifier, T=T)\n",
        "\n",
        "        self.step = 0\n",
        "\n",
        "        wandb.init(project='sde', name='noisy_classifier')\n",
        "\n",
        "        def get_logits(X, y):\n",
        "            t = self.sample_time(X.size(0)).to(device)\n",
        "\n",
        "            \"\"\"calc logits\"\"\"\n",
        "\n",
        "            return loss, pred_labels\n",
        "\n",
        "        self.set_data_generator()\n",
        "        train_generator = self.datagen.sample_train()\n",
        "        classifier.train()\n",
        "\n",
        "        self.config.training.snapshot_batch_size = 100\n",
        "        labels = np.tile(np.arange(10), (10, 1))\n",
        "        labels = torch.Tensor(labels).to(device).long().view(-1)\n",
        "\n",
        "        for iter_idx in trange(1, 1 + self.config.classifier.training_iters):\n",
        "            self.step = iter_idx\n",
        "\n",
        "            \"\"\"\n",
        "            train classifier\n",
        "            \"\"\"\n",
        "\n",
        "            if iter_idx % self.config.classifier.snapshot_freq == 0:\n",
        "                self.snapshot(labels=labels)\n",
        "\n",
        "            if iter_idx % self.config.classifier.eval_freq == 0:\n",
        "                valid_loss = 0\n",
        "                valid_accuracy = 0\n",
        "                valid_count = 0\n",
        "                classifier.eval()\n",
        "                with torch.no_grad():\n",
        "                    \"\"\"\n",
        "                    validate classifier\n",
        "                    \"\"\"\n",
        "                valid_loss = valid_loss / valid_count\n",
        "                valid_accuracy = valid_accuracy / valid_count\n",
        "                self.log_metric('cross_entropy', 'valid', valid_loss)\n",
        "                self.log_metric('accuracy', 'valid', valid_accuracy)\n",
        "                classifier.train()\n",
        "\n",
        "            if iter_idx % self.config.classifier.checkpoint_freq == 0:\n",
        "                torch.save(\n",
        "                    classifier.state_dict(),\n",
        "                    self.config.classifier.checkpoint_path\n",
        "                )\n",
        "\n",
        "        classifier.eval()\n",
        "        torch.save(\n",
        "            classifier.state_dict(),\n",
        "            self.config.classifier.checkpoint_path\n",
        "        )"
      ],
      "id": "cd0d5cc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b859b1df"
      },
      "outputs": [],
      "source": [
        "config =  create_default_cm_2_config()\n",
        "\n",
        "\n",
        "config.data.dataset = \"MNIST-colored_2\"\n",
        "config.data.image_size = 16\n",
        "config.data.num_channels = 3\n",
        "config.data.centered = True\n",
        "config.data.batch_size = 32\n",
        "config.data.norm_mean = (0.5)\n",
        "config.data.norm_std = (0.5)\n",
        "config.data.path = \"/trinity/home/a.kolesov/data/MNIST\"\n",
        "\n",
        "# model\n",
        "config.model.ch_mult = (1, 2, 2, 2)\n",
        "config.model.num_res_blocks = 4\n",
        "config.model.attn_resolutions = (16,)\n",
        "config.model.dropout = 0.1\n",
        "config.model.resamp_with_conv = True\n",
        "config.model.conditional = True\n",
        "config.model.nonlinearity = 'swish'\n",
        "diffusion = DiffusionRunner(config)"
      ],
      "id": "b859b1df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd25db74",
        "outputId": "d9d04096-6448-460d-da2e-b8b50fc1ade5",
        "colab": {
          "referenced_widgets": [
            "",
            "294bbddff2804ca39439c8ba45426c73"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:32srqn9n) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 606982<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/trinity/home/a.kolesov/Seminars/wandb/run-20241009_171800-32srqn9n/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/trinity/home/a.kolesov/Seminars/wandb/run-20241009_171800-32srqn9n/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>loss/train</td><td>0.01566</td></tr><tr><td>lr/train</td><td>0.0002</td></tr><tr><td>_runtime</td><td>1484</td></tr><tr><td>_timestamp</td><td>1728484964</td></tr><tr><td>_step</td><td>5000</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>loss/train</td><td>█▇▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr/train</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██████████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">ddpm_cont</strong>: <a href=\"https://wandb.ai/emfalafeli/sde/runs/32srqn9n\" target=\"_blank\">https://wandb.ai/emfalafeli/sde/runs/32srqn9n</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:32srqn9n). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.3 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.33<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">ddpm_cont</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/emfalafeli/sde\" target=\"_blank\">https://wandb.ai/emfalafeli/sde</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/emfalafeli/sde/runs/wbcm30vw\" target=\"_blank\">https://wandb.ai/emfalafeli/sde/runs/wbcm30vw</a><br/>\n",
              "                Run data is saved locally in <code>/trinity/home/a.kolesov/Seminars/wandb/run-20241009_174244-wbcm30vw</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "294bbddff2804ca39439c8ba45426c73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "diffusion.train()"
      ],
      "id": "bd25db74"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7c93db"
      },
      "source": [
        "**Полезные ссылки:**\n",
        "\n",
        "- Репозиторий метода https://notebooks.githubusercontent.com/view/ipynb?browser=unknown_browser&color_mode=auto&commit=73bb1b5bd6c7af6b06abc52957de76efd4e91175&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d727961622f646c2d6873652d616d692f373362623162356264366337616636623036616263353239353764653736656664346539313137352f7765656b31305f70726f626d6f64656c732f686f6d65776f726b2e6970796e62&logged_in=false&nwo=mryab%2Fdl-hse-ami&path=week10_probmodels%2Fhomework.ipynb&platform=unknown_platform&repository_id=531107718&repository_type=Repository&version=0\n",
        "\n",
        "\n",
        "- Блог с объяснениями непрерывного метода https://allanchan339.github.io/2022/12/22/Review-DDIM/"
      ],
      "id": "4b7c93db"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4a6b742"
      },
      "source": [
        "**Заключение**:\n",
        "\n",
        "1. Познакомились с теорией СДУ\n",
        "2. Разобрались с понятием маргинального распределения\n",
        "3. Вывели уравнения моментов СДУ\n",
        "4. Разобрали пайплайн непрывной диффузионной модели"
      ],
      "id": "c4a6b742"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "992a7441",
        "zHnqiZp82krW",
        "CtcdCCJpYCTz",
        "M8uihoiI2krX",
        "b8dbb296",
        "8e5e3c1d",
        "f0472491"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}