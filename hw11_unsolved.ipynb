{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NosenkoArtem/Categorical-Encoding/blob/master/hw11_unsolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6831fe55",
      "metadata": {
        "id": "6831fe55"
      },
      "source": [
        "# KV Cache - 10 баллов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29b7f3c",
      "metadata": {
        "id": "f29b7f3c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5045cc82",
      "metadata": {
        "id": "5045cc82"
      },
      "source": [
        "Представим, что у нас есть очень простая мини-LLM:\n",
        "1. Она эмбеддит токены\n",
        "2. Считает аттеншн (обычный, не multihead) токенов друг с другом с causal mask (не смотрит в будущее!)\n",
        "3. После этого выходы attention подаются в линейный слой для получения распределеняи по словарю\n",
        "\n",
        "На примере такой модели давайте попробуем имплементировать KV-Cache.\n",
        "\n",
        "Ниже за вас написан метод forward - этот метод это обычный forward нейросети, который считает attention всех токенов со всеми токенами.\n",
        "\n",
        "Вам же нужно имплементировать метод forward_kv_cache, который принимает:\n",
        "* x - тензор размерности \\[batch, seq_len = 1\\]\n",
        "* prev_output - выход модели с предыдущего шага типа Output\n",
        "\n",
        "Метод forward_kv_cache должен выполнять следующие действия:\n",
        "1. Эмбеддинг токена x\n",
        "2. Проекция x в QKV\n",
        "3. Расширение k_cache и v_cache из prev_ouptut k/v проекциями x\n",
        "4. Подсчет аттеншена между q_x и k_cache и v_cache\n",
        "5. Конкатенация аттеншена в prev_output.attention_weights\n",
        "6. Возврат logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce42ab63",
      "metadata": {
        "id": "ce42ab63",
        "outputId": "c10c0516-bd4f-4c7f-bd04-4d0abd68dba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before generation tensor([[1, 3, 0]])\n",
            "After generation no cache tensor([[1, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]])\n",
            "After generation kv cache tensor([[1, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]])\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Output:\n",
        "    logits: torch.Tensor = None\n",
        "    k_cache: torch.Tensor = None\n",
        "    v_cache: torch.Tensor = None\n",
        "    attn_weights: torch.Tensor = None\n",
        "\n",
        "\n",
        "\n",
        "class SimpleAttentionLLM(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        self.lin = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # batch x seq_len x d_model\n",
        "        q = self.W_Q(x)       # batch x seq_len x d_model\n",
        "        k = self.W_K(x)       # batch x seq_len x d_model\n",
        "        v = self.W_V(x)       # batch x seq_len x d_model\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.permute(0, 2, 1))               # batch x seq_len x seq_len\n",
        "        mask = torch.tril(torch.ones_like(attn_scores))                 # batch x seq_len x seq_len\n",
        "        attn_scores = attn_scores.masked_fill(~mask.bool(), -torch.inf) # batch x seq_len x seq_len\n",
        "        attn_weights = torch.softmax(attn_scores, dim=2)                # batch x seq_len x seq_len\n",
        "        weights_V = torch.matmul(attn_weights, v)                       # batch x seq_len x d_model\n",
        "        logits = self.lin(weights_V)                                    # batch x seq_len x vocab_size\n",
        "        return Output(\n",
        "            logits=logits,\n",
        "            k_cache=k,\n",
        "            v_cache=v,\n",
        "            attn_weights=attn_weights # batch, seq_len, seq_len\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward_kv_cache(self, x, prev_output):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 1. Проецируем x в q, k, v\n",
        "        q = self.W_Q(x)\n",
        "        k = self.W_K(x)\n",
        "        v = self.W_V(x)\n",
        "\n",
        "\n",
        "        # берем старый кэш\n",
        "        k_cache = prev_output.k_cache\n",
        "        v_cache = prev_output.v_cache\n",
        "\n",
        "        # расширяем его состояниями k, v последнего токена\n",
        "        # с помощью torch.cat\n",
        "        k_cache_new = torch.cat([k_cache, k], dim=1)\n",
        "        v_cache_new = torch.cat([v_cache, v], dim=1)\n",
        "\n",
        "        # считаем attention_score, то есть матричное умножение между q и k_cache_new\n",
        "        attn_scores = torch.matmul(q, k_cache_new.permute(0, 2, 1))\n",
        "\n",
        "\n",
        "        # считаем softmax\n",
        "        attn_weights = torch.softmax(attn_scores, dim=2)\n",
        "        # домножаем softmax на V\n",
        "\n",
        "        weights_V = torch.matmul(attn_weights, v_cache_new)\n",
        "        logits = self.lin(weights_V)\n",
        "\n",
        "        batch = x.size(0)\n",
        "        seq_len = k_cache.size(1)\n",
        "\n",
        "        attn_weights_old = prev_output.attn_weights\n",
        "        zeros_right = torch.zeros(batch, seq_len, 1)\n",
        "        # добавляем нули в аттеншене так, чтобы у старых токенов был нулевой аттеншн на новый токен\n",
        "        attn_weights_all = torch.cat((attn_weights_old, zeros_right), dim=2)\n",
        "        # Добавляем аттеншн текущего нового токена по старым\n",
        "        attn_weights_all = torch.cat((attn_weights_all, attn_weights), dim=1)\n",
        "\n",
        "        return Output(\n",
        "            logits=logits,\n",
        "            k_cache=k_cache_new,\n",
        "            v_cache=v_cache_new,\n",
        "            attn_weights=attn_weights_all\n",
        "        )\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "batch_size = 1\n",
        "seq_len = 3\n",
        "d_model = 128\n",
        "vocab_size = 7\n",
        "\n",
        "layer = SimpleAttentionLLM(d_model, vocab_size)\n",
        "for param in layer.parameters():\n",
        "    nn.init.normal_(param)\n",
        "\n",
        "x = torch.randint(0, 7, (batch_size, seq_len))\n",
        "x_copy = x.clone()\n",
        "\n",
        "print(\"Before generation\", x)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(10):\n",
        "        outputs = layer(x)\n",
        "        logits = outputs.logits\n",
        "        # берем последний токен, т.к. по нему предсказываем!\n",
        "        next_token = logits[:, -1].argmax(dim=1, keepdim=True)\n",
        "        x = torch.cat((x, next_token), dim=1) # добавляем новый токен по размерности seq_len\n",
        "\n",
        "no_cache_output = outputs\n",
        "print(\"After generation no cache\", x)\n",
        "\n",
        "\n",
        "x = x_copy.clone()\n",
        "final_tokens = x.clone()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # prefill\n",
        "    outputs = layer(x)\n",
        "    logits = outputs.logits\n",
        "    # берем последний токен, т.к. по нему предсказываем!\n",
        "    next_token = logits[:, -1].argmax(dim=1, keepdim=True)\n",
        "    # обратите внимание, что раньше было 10 шагов!\n",
        "    final_tokens = torch.cat((final_tokens, next_token), dim=1)\n",
        "    for i in range(9):\n",
        "        outputs = layer.forward_kv_cache(next_token, outputs)\n",
        "        next_token = outputs.logits[:, -1].argmax(dim=1, keepdim=True)\n",
        "        final_tokens = torch.cat((final_tokens, next_token), dim=1)\n",
        "\n",
        "print(\"After generation kv cache\", final_tokens)\n",
        "\n",
        "cache_outputs = outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be62b506",
      "metadata": {
        "id": "be62b506"
      },
      "source": [
        "Выведем матрицы attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305f232d",
      "metadata": {
        "id": "305f232d",
        "outputId": "28ee9b82-94cd-4cc4-dcea-dfd7c8e0cb4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Attention kv cache')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "axes[0].imshow(no_cache_output.attn_weights[0])\n",
        "axes[0].set_title(\"Attention no cache\")\n",
        "axes[1].imshow(cache_outputs.attn_weights[0])\n",
        "axes[1].set_title(\"Attention kv cache\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Полная очистка окружения\n",
        "!pip uninstall -y torch vllm transformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/torch*\n",
        "\n",
        "# 2. Установка совместимых версий\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install vllm==0.3.3  # Версия, совместимая с PyTorch 2.1\n",
        "!pip install hf_xet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B2mxf42lygF",
        "outputId": "ba1b1bda-c24d-49a6-ddef-17c8feb7e7da"
      },
      "id": "4B2mxf42lygF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.1.2+cu118\n",
            "Uninstalling torch-2.1.2+cu118:\n",
            "  Successfully uninstalled torch-2.1.2+cu118\n",
            "Found existing installation: vllm 0.3.3\n",
            "Uninstalling vllm-0.3.3:\n",
            "  Successfully uninstalled vllm-0.3.3\n",
            "Found existing installation: transformers 4.52.3\n",
            "Uninstalling transformers-4.52.3:\n",
            "  Successfully uninstalled transformers-4.52.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "Collecting torchvision==0.16.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.16.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "Collecting torchaudio==2.1.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2025.3.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (1.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "outlines 0.0.34 requires transformers, which is not installed.\n",
            "peft 0.15.2 requires transformers, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.2+cu118 torchaudio-2.1.2+cu118 torchvision-0.16.2+cu118\n",
            "Collecting vllm==0.3.3\n",
            "  Using cached vllm-0.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (1.11.1.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (5.9.5)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (2.46.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (1.26.0)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (2.1.2+cu118)\n",
            "Collecting transformers>=4.38.0 (from vllm==0.3.3)\n",
            "  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: xformers==0.0.23.post1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (0.0.23.post1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (0.115.12)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (0.34.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (2.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (0.22.0)\n",
            "Requirement already satisfied: pynvml==11.5.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (11.5.0)\n",
            "Requirement already satisfied: triton>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (2.1.0)\n",
            "Requirement already satisfied: outlines>=0.0.27 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (0.0.34)\n",
            "Requirement already satisfied: cupy-cuda12x==12.1.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.3.3) (12.1.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x==12.1.0->vllm==0.3.3) (0.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->vllm==0.3.3) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->vllm==0.3.3) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->vllm==0.3.3) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->vllm==0.3.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->vllm==0.3.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->vllm==0.3.3) (2025.3.2)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (0.3.3)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (1.2.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (3.1.1)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (5.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (1.15.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (0.60.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (1.5.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (4.23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.27->vllm==0.3.3) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->vllm==0.3.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->vllm==0.3.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->vllm==0.3.3) (0.4.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.3.3) (8.2.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.3.3) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.3.3) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.3.3) (5.29.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.3.3) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->vllm==0.3.3) (0.31.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->vllm==0.3.3) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->vllm==0.3.3) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->vllm==0.3.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0->vllm==0.3.3) (4.67.1)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->vllm==0.3.3) (0.46.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.3.3) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.3.3) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.3.3) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.3.3) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.3.3) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.3.3) (15.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi->vllm==0.3.3) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2->vllm==0.3.3) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines>=0.0.27->vllm==0.3.3) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines>=0.0.27->vllm==0.3.3) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines>=0.0.27->vllm==0.3.3) (0.25.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->outlines>=0.0.27->vllm==0.3.3) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outlines>=0.0.27->vllm==0.3.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outlines>=0.0.27->vllm==0.3.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->outlines>=0.0.27->vllm==0.3.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outlines>=0.0.27->vllm==0.3.3) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2->vllm==0.3.3) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->vllm==0.3.3) (1.3.1)\n",
            "Using cached vllm-0.3.3-cp311-cp311-manylinux1_x86_64.whl (44.4 MB)\n",
            "Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
            "Installing collected packages: transformers, vllm\n",
            "Successfully installed transformers-4.52.3 vllm-0.3.3\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U ftfy regex timm\n",
        "# !pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "-FqMDYLIc3Cn"
      },
      "id": "-FqMDYLIc3Cn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.26.0\n",
        "# !pip install -q vllm==0.3.3.post1\n",
        "# !torchvision==0.17.1\n",
        "# !pip install transformers==4.41.0\n",
        "!export CUDA_HOME=/usr/local/cuda  # Важно для совместимости"
      ],
      "metadata": {
        "id": "DL_dh0aGZbfd"
      },
      "id": "DL_dh0aGZbfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPVisionModel\n",
        "print(\"CLIP доступен:\", CLIPVisionModel is not None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-jvlCoQeUQm",
        "outputId": "769a930c-6903-46a9-f978-815e9662de0d"
      },
      "id": "A-jvlCoQeUQm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP доступен: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Команда запуска (подставьте свою модель)\n",
        "server_cmd = [\n",
        "    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "    \"--model\", \"unsloth/gemma-2b-it\",         # Лёгкая модель для Colab\n",
        "    \"--tensor-parallel-size\", \"1\",           # 1 GPU\n",
        "    \"--max-num-batched-tokens\", \"2048\",      # Лимит токенов\n",
        "    \"--max-model-len\", \"2048\",               # Макс. длина контекста\n",
        "    \"--dtype\", \"float16\",                    # Для экономии памяти\n",
        "    \"--port\", \"8000\"\n",
        "]\n",
        "\n",
        "# Запуск в фоне с перенаправлением логов\n",
        "with open(\"vllm.log\", \"w\") as log_file:\n",
        "    server_process = subprocess.Popen(\n",
        "        server_cmd,\n",
        "        stdout=log_file,\n",
        "        stderr=subprocess.STDOUT\n",
        "    )\n",
        "\n",
        "# Дадим серверу 60 секунд на запуск\n",
        "time.sleep(60)"
      ],
      "metadata": {
        "id": "8Isi4cyAZi5A"
      },
      "id": "8Isi4cyAZi5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Проверка здоровья сервера\n",
        "health_url = \"http://localhost:8000/health\"\n",
        "try:\n",
        "    response = requests.get(health_url)\n",
        "    print(f\"Server status: {response.status_code}\")\n",
        "    print(response.json())\n",
        "except Exception as e:\n",
        "    print(\"Ошибка подключения:\", e)\n",
        "    !cat vllm.log  # Показать логи при ошибке"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhPIXSckZoOl",
        "outputId": "939eaf37-dfd0-41c9-b8b5-a0e07c3dd119"
      },
      "id": "nhPIXSckZoOl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server status: 200\n",
            "Ошибка подключения: Expecting value: line 1 column 1 (char 0)\n",
            "INFO 05-28 20:50:46 api_server.py:228] args: Namespace(host=None, port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, root_path=None, middleware=[], model='unsloth/gemma-2b-it', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='float16', kv_cache_dtype='auto', max_model_len=2048, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=2048, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
            "WARNING 05-28 20:50:47 config.py:618] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 05-28 20:50:47 llm_engine.py:87] Initializing an LLM engine with config: model='unsloth/gemma-2b-it', tokenizer='unsloth/gemma-2b-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
            "INFO 05-28 20:50:53 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
            "INFO 05-28 20:51:19 llm_engine.py:357] # GPU blocks: 23386, # CPU blocks: 14563\n",
            "INFO 05-28 20:51:22 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 05-28 20:51:22 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 05-28 20:51:30 model_runner.py:756] Graph capturing finished in 9 secs.\n",
            "INFO 05-28 20:51:32 serving_chat.py:302] Using default chat template:\r\n",
            "INFO 05-28 20:51:32 serving_chat.py:302] {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\r\n",
            "INFO 05-28 20:51:32 serving_chat.py:302] ' + message['content'] | trim + '<end_of_turn>\r\n",
            "INFO 05-28 20:51:32 serving_chat.py:302] ' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\r\n",
            "INFO 05-28 20:51:32 serving_chat.py:302] '}}{% endif %}\n",
            "INFO:     Started server process [11651]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     127.0.0.1:46926 - \"GET /health HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
        "                  \"model\": \"unsloth/gemma-2b-it\",\n",
        "                  \"prompt\": \"Hello there\",\n",
        "                  \"max_tokens\": 20,\n",
        "                  \"temperature\": 0.8\n",
        "              })\n",
        "print(r.json())\n",
        "print(r.json()[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFQT29a1q7sj",
        "outputId": "d0d1d9c3-2de4-4978-edbb-8eef0b42082c"
      },
      "id": "GFQT29a1q7sj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-dd21fae22c8c475aa0b9eb8f2676d89f', 'object': 'text_completion', 'created': 2704, 'model': 'unsloth/gemma-2b-it', 'choices': [{'index': 0, 'text': ', I am interested in learning more about your work and how I can get in touch with you.', 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 3, 'total_tokens': 23, 'completion_tokens': 20}}\n",
            ", I am interested in learning more about your work and how I can get in touch with you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326b101e",
      "metadata": {
        "id": "326b101e"
      },
      "outputs": [],
      "source": [
        "!pip install -q vllm triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Запуск сервера в фоне\n",
        "# import subprocess\n",
        "# vllm_process = subprocess.Popen([\n",
        "#     \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "#     \"--model\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "#     \"--trust-remote-code\",\n",
        "#     \"--dtype\", \"half\",\n",
        "#     \"--max-model-len\", \"8192\"\n",
        "# ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
      ],
      "metadata": {
        "id": "sUCxbAfrWJBt"
      },
      "id": "sUCxbAfrWJBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# response = requests.get(\"http://localhost:8000/health\")\n",
        "# print(\"Status:\", response.status_code)"
      ],
      "metadata": {
        "id": "I4mEKdXoX4eZ"
      },
      "id": "I4mEKdXoX4eZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "76e45fc6",
      "metadata": {
        "id": "76e45fc6"
      },
      "source": [
        "По возможности данное задание необходимо выполнять на сервере, чтобы было удобно запускать параллельно фреймворк и нагрузочное тестирование. Для удобства работы в jupyter сделан следующий трюк"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b6fba6",
      "metadata": {
        "id": "d2b6fba6"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# import multiprocessing\n",
        "# import subprocess\n",
        "\n",
        "# def start_vllm_server():\n",
        "#     # This function will run the `vllm` server command\n",
        "#     cmd = [\"vllm\", \"serve\", \"unsloth/gemma-2b-it\", \"--dtype\", \"half\"]\n",
        "#     subprocess.run(cmd)\n",
        "\n",
        "\n",
        "# server_process = multiprocessing.Process(target=start_vllm_server)\n",
        "# server_process.start()\n",
        "# time.sleep(60)\n",
        "# print(\"we are probably ready\")\n",
        "\n",
        "# import requests\n",
        "\n",
        "# r = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
        "#                   \"model\": \"unsloth/gemma-2b-it\",\n",
        "#                   \"prompt\": \"Hello there\",\n",
        "#                   \"max_tokens\": 20,\n",
        "#                   \"temperature\": 0.8\n",
        "#               })\n",
        "# print(r.json())\n",
        "# print(r.json()[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad31f5df",
      "metadata": {
        "id": "ad31f5df"
      },
      "source": [
        "# Нагрузочное тестирование - 25 баллов\n",
        "\n",
        "20 баллов - узнать, сколько запросов в секунду выдержит VLLM сервинг модели при ограничении latency в 5 секунд.\n",
        "Можно использовать самописную функцию через multiprocessing/threading, можно использовать любой готовый инструмент.\n",
        "\n",
        "В качестве пейлоадов предлагается брать тексты длины 100-128 и генерировать к ним не более 10 токенов. Сами пейлоады можно взять из любого датасета, например https://huggingface.co/datasets/Intel/orca_dpo_pairs.\n",
        "\n",
        "Об аргументах vllm serve можно почитать в документации https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
        "\n",
        "\n",
        "5 баллов - если сможете посчитать на этом пейлоаде ttft - time to first token, то есть сколько занимает prefill стадия генерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e05e253",
      "metadata": {
        "id": "1e05e253"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "08c334b3",
      "metadata": {
        "id": "08c334b3"
      },
      "source": [
        "# Часть 2\n",
        "Далее предоставлено 2 варианта выполнения задания. Баллы будут начисляться только на один из них\n",
        "## Вариант 1. Квантизация - 15 баллов\n",
        "Квантизируйте модель в VLLM любым доступным способом, напишите, как сократились затраты памяти и как изменилась скорость инференса. Обязательно укажите, на какой видеокарте проводились замеры!\n",
        "Не забудьте про то, что квантизировать можно и kv cache.\n",
        "\n",
        "Внимательно проверьте и убедитесь, что ваш ускоритель поддержан в https://docs.vllm.ai/en/latest/quantization/supported_hardware.html\n",
        "\n",
        "## Вариант 2. Батчевалка - 15 баллов\n",
        "Предлагается написать сервер на питоне, который поддерживал бы батчевание запросов.\n",
        "\n",
        "Сервер принимает POST запрос с телом вида\n",
        "\n",
        "```json\n",
        "{\"text\": \"Hello there\"}\n",
        "```\n",
        "\n",
        "\n",
        "Необходимо написать сервер, который:\n",
        "1. Имел бы возможность работать с несколькими клиентами за раз (не блокировался бы на обработку одного запроса). Для этого можно использовать async, gevent, треды и т.д.\n",
        "2. Использовал бы батчовую обработку следующим образом:\n",
        "если пришло несколько запросов (для примера 2)\n",
        "\n",
        "```json\n",
        "запрос 1\n",
        "{\"text\": \"Hello there \"}\n",
        "запрос 2\n",
        "{\"text\": \"handsome\"}\n",
        "```\n",
        "\n",
        "то каждый клиент получал бы в ответ конкатенацию этих запросов (в произвольном порядке), т.е. оба клиента получили бы ответ\n",
        "```json\n",
        "{\"text\": \"Hello there handsome\"}\n",
        "или\n",
        "{\"text\": \"handsomeHello there \"}\n",
        "```\n",
        "\n",
        "Сервер должен иметь 2 конфигурируемых параметра:\n",
        "1. Максимальный размер батча, который он может обработать\n",
        "2. Максимальное время ожидания, которое ждет сэмпл перед обработкой. Т.е. если у нас батч размера 5, а у нас всего один сэмпл, и прошло максимальное время ожидания - этот сэмпл попадает в батч один и обрабатывается один.\n",
        "\n",
        "Для хранения данных в очереди можно использовать queue.Queue или любой другой удобный способ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe94205",
      "metadata": {
        "id": "1fe94205"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}