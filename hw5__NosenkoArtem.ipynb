{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NosenkoArtem/Categorical-Encoding/blob/master/hw5__NosenkoArtem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TDKBmIM2Alp9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDKBmIM2Alp9",
        "outputId": "ba79a124-cd0e-421e-8d48-e422f60c06f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wTU-fwZygvUr",
      "metadata": {
        "id": "wTU-fwZygvUr"
      },
      "source": [
        "# Домашнее задание: Доступные LLM\n",
        "\n",
        "В этом домашнем задании мы познакомимся с библиотекой transformers и разберемся, как можно open source пользоваться моделями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11861b12",
      "metadata": {
        "id": "11861b12"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
      "metadata": {
        "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"
      },
      "source": [
        "# Знакомство с Transformers - 35 баллов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3df5693",
      "metadata": {
        "id": "a3df5693"
      },
      "source": [
        "## Создание модели и предсказание следующего токена - 5 баллов\n",
        "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и олучить следующий токен через жадную генерацию!\n",
        "\n",
        "1. Для создания модели используйте метод `from_pretrained` у `AutoModelForCausalLM` и `AutoTokenizer`;\n",
        "2. Чтобы токенизировать текст вызовите `tokenizer(text, return_tensors=\"pt\")`, тогда вы получите словарь тензоров\n",
        "3. Передайте его ключи и значения в качестве аргументов в `__call__` (forward) метод модели и получите logits размерности \\[batch_size, seq_len, vocab_size\\]\n",
        "4. По logits предскажите следующий токен и детокенизируйте его с помощью `tokenizer.decode`\n",
        "\n",
        "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6ab0d8-60a0-4def-a45c-b1becf4930e1",
      "metadata": {
        "id": "5a6ab0d8-60a0-4def-a45c-b1becf4930e1"
      },
      "outputs": [],
      "source": [
        "def move_to_device(inputs, device):\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = v.to(device)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YMp8daVEhYKE",
      "metadata": {
        "id": "YMp8daVEhYKE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device) # Ваш код здесь\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device) # ваш код здесь\n",
        "\n",
        "\n",
        "text = \"This is a sample text\"\n",
        "\n",
        "# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n",
        "# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n",
        "# По этому тензору нужно предсказать следующее слово!\n",
        "\n",
        "inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "imputs_ids = inputs['input_ids']\n",
        "mask = inputs['attention_mask']\n",
        "\n",
        "outputs = model(imputs_ids)\n",
        "logits = outputs['logits']\n",
        "next_token_idx: int = torch.argmax(logits, dim=-1)[:, -1].item()\n",
        "\n",
        "\n",
        "next_token = tokenizer.decode([next_token_idx])\n",
        "\n",
        "assert next_token.strip() == \"file\"\n",
        "\n",
        "\n",
        "# ---- Конец кода ----\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4D_WW2XBZxE",
        "outputId": "0214fa25-f0f6-4d55-e698-510c425d91fc"
      },
      "id": "h4D_WW2XBZxE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2393"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6809813",
      "metadata": {
        "id": "e6809813"
      },
      "source": [
        "## Используем Generate - 5 баллов\n",
        "\n",
        "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
        "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
        "\n",
        "Для генерации нескольких токенов сразу есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "\n",
        "Ваша задача написать для модели выше генерацию по тексту с:\n",
        "* Температурой - 0.9\n",
        "* Top-K - 20\n",
        "* Repetition Penalty (Frequency Penalty) - 1.2\n",
        "* максимальное число новых токенов - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_generation = {\n",
        "    'max_new_tokens' : 10,\n",
        "    'do_sample' : True,\n",
        "    'temperature' : 0.9,\n",
        "    'top_k' : 20,\n",
        "    'repetition_penalty' : 1.2\n",
        "}"
      ],
      "metadata": {
        "id": "VmOorwPo-9lP"
      },
      "id": "VmOorwPo-9lP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b62dbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6b62dbf",
        "outputId": "18462188-921a-42ed-ab6f-40f63e88a08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "text = \"This is still a sample text, but\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "results = []\n",
        "for i in range(10):\n",
        "    # ---- Ваш код здесь ----\n",
        "    gens = model.generate(\n",
        "        **inputs,\n",
        "        **params_generation\n",
        "    )\n",
        "    genertaion: str = tokenizer.decode(gens.view(-1)) # сгенерированный текст\n",
        "    results.append(genertaion)\n",
        "    # ---- Конец кода ----\n",
        "\n",
        "\n",
        "assert len(set(results)) > 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DUzlS4FApVJ",
        "outputId": "f8070a16-157f-4888-9bf1-10f6e33e7ef3"
      },
      "id": "2DUzlS4FApVJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is still a sample text, but if you want to see the full output then use',\n",
              " \"This is still a sample text, but it's not all that difficult to figure out.\",\n",
              " \"This is still a sample text, but it should be quite clear that we've seen the\",\n",
              " \"This is still a sample text, but it's more than just another font and you can\",\n",
              " \"This is still a sample text, but it's not 100% complete as of yet;\",\n",
              " 'This is still a sample text, but the original looks rather crudely designed and quite difficult',\n",
              " \"This is still a sample text, but I'll update soon. This was my first time\",\n",
              " 'This is still a sample text, but the original image was taken just last week.\\n',\n",
              " 'This is still a sample text, but it might be worth the time to look at more',\n",
              " \"This is still a sample text, but the code isn't too obvious. But there are\"]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
      "metadata": {
        "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"
      },
      "source": [
        "## Generate Batched - 5\n",
        "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
        "\n",
        "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
        "\n",
        "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
        "\n",
        "Тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1]\n",
        "    [5, 6,  7,  1,  2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Представим, что мы сгенерировали еще один токен, тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1, 7]\n",
        "    [5, 6,  7,  1,  2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
        "Тогда исходная последовательность будет:\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2]\n",
        "    [ 5,  6,  7, 1, 2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "и после генерации следующего токена\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2, 7]\n",
        "    [ 5,  6,  7, 1, 2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
        "\n",
        "Для этого нам придется использовать параметр padding_side=\"left\" в конструкторе токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
      "metadata": {
        "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name,\n",
        "                                          padding_side='left') # ваш код здесь\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# ---- Конец кода ----\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
      "metadata": {
        "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
        "\n",
        "\n",
        "# ---- Ваш код здесь ----\n",
        "\n",
        "# Внимание! В данном задании нужна жадная генерация!\n",
        "\n",
        "# Соберите оба текста в один батч и положите результаты генерации в\n",
        "# batched_generations\n",
        "batched_generations: List[str] = []\n",
        "\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    next_tokens = tokenizer.decode(model.generate(**inputs, max_new_tokens=10)[0])\n",
        "    batched_generations.append(next_tokens)\n",
        "\n",
        "# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n",
        "# в single_generations\n",
        "single_generations: List[str] = []\n",
        "\n",
        "...\n",
        "\n",
        "# ---- Конец кода ----\n",
        "\n",
        "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
        "for s, b in zip(batched_generations, single_generations):\n",
        "    assert s == b\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
      "metadata": {
        "id": "f5da008c-3653-40d5-89ba-cd831352fd3d"
      },
      "source": [
        "## Скоринг, Perplixity - 10 баллов\n",
        "\n",
        "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
        "\n",
        "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
        "\n",
        "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
        "\n",
        "```\n",
        "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
        "```\n",
        "\n",
        "Т.е. это вероятность слова при условии его левого контекста.\n",
        "Зачастую ее обозначают как $\\P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
        "Эти вероятности можно взять из выходного вектора!\n",
        "\n",
        "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
        "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
        "\n",
        "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
        "\n",
        "В этом задании нужно:\n",
        "1. Посчитать вероятность **text**\n",
        "2. Посчитать перплексию **text**\n",
        "\n",
        "Еще одна важная деталь:\n",
        "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
        "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
        "\n",
        "$$ p = p_1 * p_2 * p_3 $$\n",
        "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
        "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
        "\n",
        "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
      "metadata": {
        "id": "e1c7ba39-a451-43a2-ac55-629c99259abe"
      },
      "outputs": [],
      "source": [
        "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
        "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
        "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# ---- Ваш код здесь ----\n",
        "inputs = tokenizer(text, ...)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(...).logits\n",
        "    ...\n",
        "    # ваш код здесь!\n",
        "    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n",
        "    # 2. Превращаем logits в log_probs\n",
        "    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n",
        "    # для этого нам поможет torch.gather\n",
        "    # 4. Считаем вероятности и perplexity!\n",
        "\n",
        "# ---- Конец кода ----\n",
        "\n",
        "\n",
        "print(text_P)\n",
        "print(ppl)\n",
        "\n",
        "# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f244eac-7cb1-4689-8adc-46662891e657",
      "metadata": {
        "id": "4f244eac-7cb1-4689-8adc-46662891e657"
      },
      "source": [
        "## Вопросы - 5 баллов\n",
        "\n",
        "**Ответьте на вопросы текстом прямо здесь!**\n",
        "\n",
        "\n",
        "1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n",
        "2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CwqRLxa2mPku",
      "metadata": {
        "id": "CwqRLxa2mPku"
      },
      "outputs": [],
      "source": [
        "# ваш ответ тут\n",
        "# ---- Ваш код здесь ----\n",
        "\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
      "metadata": {
        "id": "5ddd5038-620b-48bb-bbc1-db3729141d78"
      },
      "source": [
        "# Chat-Models - 20 баллов\n",
        "\n",
        "Теперь мы познакомимся с chat-моделями, т.е. с моделями, которые предоставляют возможность общаться с ними как с ассистентом. Эти модели не просто продолджают текст слева-направо, а дают ответ на заданный вопрос.\n",
        "\n",
        "## Формат - 5 баллов\n",
        "\n",
        "Все chat-модели принимают ответ в особом формате, который позволяет им различать, кому принадлежит фраза - пользователю (user) или модели (assistant).\n",
        "Давайте попробуем подать модели вопрос без какого-либо форматирования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5fe593-63a8-406d-9678-6d805c180670",
      "metadata": {
        "id": "7f5fe593-63a8-406d-9678-6d805c180670"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", torch_dtype=torch.half).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
      "metadata": {
        "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b"
      },
      "outputs": [],
      "source": [
        "text = \"hello how are you\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "for i in range(5):\n",
        "    print(tokenizer.decode(model.generate(**move_to_device(inputs, device), max_new_tokens=20, use_cache=True, do_sample=True, pad_token_id=tokenizer.eos_token_id)[0]))\n",
        "    print(\"====\" * 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
      "metadata": {
        "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc"
      },
      "source": [
        "Видим, что текст зачастую разламывается:\n",
        "1. Иногда модель продолжает текст как базовая LLM\n",
        "2. Иногда пытается придумать роли спикерам и трасформироваться в формат диалога\n",
        "3. Иногда просто выдает бессвязный текст.\n",
        "\n",
        "Это происходит потому, что формат входных данных сильно отличается от того, что модель видела на обучении.\n",
        "Как мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec7ca96",
      "metadata": {
        "id": "fec7ca96"
      },
      "outputs": [],
      "source": [
        "prefix = tokenizer.apply_chat_template(\n",
        "    conversation=\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, who always helps user\"},\n",
        "        {\"role\": \"user\", \"content\": \"How to learn about LLMs?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"You can always attend deepschool!\"},\n",
        "        {\"role\": \"user\", \"content\": \"Thank you!\"},\n",
        "    ],\n",
        "    tokenize=False)\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ew_7ZmE7-miE",
      "metadata": {
        "id": "ew_7ZmE7-miE"
      },
      "source": [
        "Как мы видим в тексте ходы и роли разделены тэгом `<|im_start|>`. В таком формате модель училась поддерживать диалог. Давайте отформатируем следующий диалог и подадим его в генерацию модели. Подробнее про apply_chat_template можно прочитать в [туториале](https://huggingface.co/docs/transformers/main/en/chat_templating#applychattemplate). Обратите внимание на опцию add_generation_prompt! Эта опция добавляет текст таким образом, чтобы в конце была очередь генерировать текст от лица модели. Давайте попробуем собрать диалог и сгенерировать моделью ответ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
      "metadata": {
        "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"hello\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love you\"},\n",
        "]\n",
        "\n",
        "\n",
        "# ---- Ваш код здесь ----\n",
        "prefix = tokenizer.apply_chat_template(...)\n",
        "# ---- Конец кода ----\n",
        "reference = \"\"\"<|im_start|>system\n",
        "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
        "<|im_start|>user\n",
        "hello<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I'm good. How can I help you today<|im_end|>\n",
        "<|im_start|>user\n",
        "I love you<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "assert prefix.strip() == reference.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
      "metadata": {
        "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "inputs = tokenizer(prefix, ...)\n",
        "model.generate...\n",
        "print(...)\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
      "metadata": {
        "id": "a72482f3-c296-46f3-851c-57b4f91a717b"
      },
      "source": [
        "# Benchmark - 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
      "metadata": {
        "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6"
      },
      "source": [
        "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
        "* question - вопрос\n",
        "* choices - варианты ответа\n",
        "* answer - номер правильного ответа (нумерация с нуля)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
      "metadata": {
        "id": "530d1721-6623-4ca6-816c-d4f90203ceb2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
        "mmlu[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wafeDm4KB6lI",
      "metadata": {
        "id": "wafeDm4KB6lI"
      },
      "source": [
        "Наша задача здесь - выбрать моделью один из четырех ответов и получить точность больше 0.25.\n",
        "\n",
        "Есть несколько вариантов, как это делать. **Эти варианты отличаются по сложности и являются взаимоисключающими. За подход с генерацией можно получить максимум 5 баллов, за подход со скорингом по 1 сэмплу можно получить только 10 баллов, а за подход со скорингом батчей можно получить все 15 баллов**\n",
        "\n",
        "### Генерация ответа - 5 баллов\n",
        "\n",
        "Можно генерировать ответ напрямую. Для этого нужно:\n",
        "1. Составить историю диалога из qeustion и choices с помощью messages и apply_chat_template\n",
        "1. Сгенерировать ответ\n",
        "1. Соотнести сгенерированный ответ с одним из вариантов ответа\n",
        "\n",
        "У этого подхода есть один важный недостаток - модель могут сгенерировать ответ, не являющийся одним из заданных вариантов ответа. Соотнесение такой генерации с ответом решается эвристиками и скорее всего приведет к множеству ошибок.\n",
        "\n",
        "\n",
        "### Скоринг по сэмплам - 10 баллов\n",
        "\n",
        "У нас есть вопрос и 4 варианта ответа в 4 набора messages и подсчитать вероятность $$P(choices_i | question)$$, то есть условную вероятность каждого ответа при заданном вопросе, т.е. сделать то же самое, что мы делали в задаче про вероятность текста и perplexity.\n",
        "\n",
        "1. Берем текст и вариант ответа, собираем из них промпт c функции `sample_to_texts` (проще будет в этом задании обойтись без apply_chat_template)\n",
        "2. Подаем это в модель через `model(**inputs)`, берем в выходах `logits`. С помощью logits и input_ids считаем вероятность токенов, которые мы подали модели.\n",
        "3. Здесь опционально можно считать как вероятность всего текста, так и только вероятность $$P(choices_i | question)$$ Т.к. для всех 4х вариантов ответа у нас общий префикс, то его вероятность будет общей константной для всех ответов.\n",
        "4. Выбираем ответ, которому была дана наибольшая вероятность нашей моделью.\n",
        "\n",
        "В этом варианте легко получить номер ответа, который модель оценивает выше и не нужно применять эвристики.\n",
        "\n",
        "### Скоринг батчами - 15 баллов\n",
        "\n",
        "Этот вариант отличается от предыдущего только тем, что нужно скорить за раз не один сэмпл и 4 ответа к нему, а несколько сэмплов за раз, т.е. обрабатывать данные батчом.\n",
        "\n",
        "Дополнительная сложность этого варианта заключается в том, что у нас возникают сэмплы различной длины, которые мы добиваем паддингами. **Паддинги не нужно учитывать при подсчете вероятностей, это служебные токены!**\n",
        "\n",
        "Чтобы вероятность паддингов не влияла на итоговую вероятность текста, можно на этапе, где у вас подсчитаны все вероятности токенов (вместе с паддингами) взять `inputs[\"attention_mask\"]` и \"занулить\" по нему вероятности паддингов (если вы считаете log_probs, если вы честно умножаете вероятности, тогда вероятности паддингов нужно поставить равными единице).\n",
        "\n",
        "В качестве проверки точности можете проверить, что качество с батчом размера 1 не сильно отличается от батча размера 3 (не более, чем на 0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc14ce5-267d-40da-b35a-193c60cc68ca",
      "metadata": {
        "id": "efc14ce5-267d-40da-b35a-193c60cc68ca"
      },
      "outputs": [],
      "source": [
        "def sample_to_texts(sample):\n",
        "    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n",
        "\n",
        "def calc_acc(p, y):\n",
        "    assert len(p) == len(y)\n",
        "    return sum(pi == yi for pi, yi in zip(p, y)) / len(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "230aed9b-32af-4a9d-8615-c2a2ddb864b2",
      "metadata": {
        "id": "230aed9b-32af-4a9d-8615-c2a2ddb864b2"
      },
      "outputs": [],
      "source": [
        "y_true = [sample[\"answer\"] for sample in mmlu]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccb1953-cf28-4629-9786-4fb71c178ac8",
      "metadata": {
        "id": "dccb1953-cf28-4629-9786-4fb71c178ac8"
      },
      "source": [
        "Считаем вероятности по одному question и choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hlw878TTFowB",
      "metadata": {
        "id": "hlw878TTFowB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "all_prompts = sum([sample_to_texts(mmlu[i]) for i in range(len(mmlu))], [])\n",
        "assert len(all_prompts) == 400\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "...\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66308e6",
      "metadata": {
        "id": "c66308e6"
      },
      "source": [
        "## Ответьте на следующие вопросы (5 баллов в сумме):\n",
        "1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n",
        "2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\n",
        "Стоит ли по-вашему добавлять эти метки?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f811187-7b10-4382-a6f5-ebec7afa8125",
      "metadata": {
        "id": "1f811187-7b10-4382-a6f5-ebec7afa8125"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "...\n",
        "# ---- Конец кода ----"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}